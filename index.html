<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML+RDFa 1.0//EN" "http://www.w3.org/MarkUp/DTD/xhtml-rdfa-1.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
    xmlns:owl="http://www.w3.org/2002/07/owl#"
    xmlns:rsa="http://www.w3.org/ns/auth/rsa#"
    xmlns:cert="http://www.w3.org/ns/auth/cert#"
    xmlns:dcterms="http://purl.org/dc/terms/"
    xmlns:foaf="http://xmlns.com/foaf/0.1/"
    xmlns:v="http://www.w3.org/2006/vcard/ns#"
    xmlns:cc="http://creativecommons.org/ns#"
    xmlns:dbr="http://dbpedia.org/resource/"
    xmlns:dbp="http://dbpedia.org/property/"
    xmlns:sioc="http://rdfs.org/sioc/ns#"
    xmlns:wgs="http://www.w3.org/2003/01/geo/wgs84_pos#"
    xmlns:cal="http://www.w3.org/2002/12/cal/ical#"
    xmlns:org="http://www.w3.org/ns/org#"
    xmlns:biblio="http://purl.org/net/biblio#"
    xmlns:book="http://purl.org/NET/book/vocab#"
    xmlns:ov="http://open.vocab.org/terms/"
    xmlns:this="http://csarven.ca/statistical-linked-dataspace"
    xml:lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
        <title>Statistical Linked Dataspace</title>
        <meta name="description" content=""/>
        <link rel="stylesheet" type="text/css" media="all" href="display.css"/>
    </head>

    <body about="[this:]" typeof="foaf:Document sioc:Post biblio:Paper" class="hfeed journal">
        <div id="wrap">
            <div class="hentry">
                <h1 property="dcterms:title" class="entry-title">On the design of dataspaces for statistical Linked Data</h1>

                <div id="authors">
                    <dl>
                        <dt>Authors</dt>
                        <dd class="entry-author"><a about="[this:]" rel="dcterms:creator dcterms:publisher dcterms:contributor" href="http://csarven.ca/#i">Sarven Capadisli</a><sup><a href="#author_email_1">1</a></sup> <sup><a href="#author_org_a">a</a></sup></dd>
                    </dl>

                    <ul id="author_emails">
                        <li id="author_email_1"><sup>1</sup><a about="http://csarven.ca/#i" rel="foaf:mbox" href="mailto:info@csarven.ca" class="author_email">info@csarven.ca</a></li>
                    </ul>

                    <ul id="author_orgs">
                        <li id="author_org_a"><sup>a</sup><span about="http://csarven.ca/#i" rel="org:memberOf" resource="http://deri.ie/"/><a href="http://deri.ie/">Digital Enterprise Research Institute</a>, <a href="http://nuigalway.ie/">NUI, Galway</a>, Ireland</li>
                    </ul>
                </div>

                <div id="abstract" class="entry-summary">
                    <h2>Abstract</h2>
                    <p property="dcterms:abstract" datatype="">~The story here is ..</p>
                </div>

                <div property="dcterms:description" id="content" class="entry-content">
                    <div id="document-identifier">
                        <h2>ID</h2>
                        <p><code>http://csarven.ca/statistical-linked-dataspace</code></p>
                    </div>

                    <div id="keywords">
                        <h2>Keywords</h2>
                        <ul about="[this:]" rel="dcterms:subject">
                            <li><a resource="http://dbpedia.org/resource/Linked_Data" href="http://en.wikipedia.org/wiki/Linked_Data">Linked Data</a></li>
                            <li><a resource="http://dbpedia.org/resource/Data_modeling" href="http://en.wikipedia.org/wiki/Data_modeling">Data modeling</a></li>
                            <li><a resource="http://dbpedia.org/resource/Knowledge_management" href="http://en.wikipedia.org/wiki/Knowledge_management">Knowledge management</a></li>
                            <li><a resource="http://dbpedia.org/resource/Statistics" href="http://en.wikipedia.org/wiki/Statistics">Statistics</a></li>
                            <li><a resource="http://dbpedia.org/resource/Life_cycle" href="http://en.wikipedia.org/wiki/Life_cycle">Life cycle</a></li>
                        </ul>
                    </div>

                    <h2>Acknowledgements</h2>

                    <h2>Contents</h2>
                    <ul>
                        <li>...</li>
                    </ul>


                    <h2 id="introduction">Introduction</h2>
                    <h3 id="problem-statement">Problem statement</h3>
                    <p>Convenient management of dataspaces for statistical Linked Data remains to be costly and lacks integrated and automated tool support. The design considerations and requirements are therefore at the forefront for reasonable functionality of these dataspace platforms.</p>

                    <p>Access to statistical data, primarily in the public sector has exploded in recent years. While these initiatives provide new opportunities to get insights on societies, management of the dataspaces are consequently confronted with new challenges. As centralized dataspaces are now faced with more heterogeneous data collections, with varying quality, solutions which employ the Linked Data principles appear to be promising.</p>

                    <p>However, due to a range of technical challenges, development teams often face low-level repetitive data management tasks with partial tooling at their disposal. These challenges on the surface include: addressing data integration, synchronisation, and access. Within the context of statistical data, the expectations from these dataspaces is that, a Linked Data tool-chain is utilized, and the data is accessible to both humans as well as the machines.</p>


                    <h3 id="hypothesis">Hypothesis</h3>
                    <p>It is contended that the deployment of statistical Linked Data comes with a specific set of requirements. The degree in which the requirements are accomplished predetermines the dataspace's usefulness for data consumers. Therefore, the hypothesis of this document is if a specific set of requirements for building and managing dataspaces for statistical Linked Data is possible, and the development of missing parts to do some of the processes better by reducing cost and making the dataspace more useful.</p>

                    <h3 id="contributions">Contributions</h3>
                    <p>With preference to minimizing developer intervention wherever possible, the contributions herein are within the expectations of working in Linked Dataspace and providing read access to data respectively. The set of design considerations and requirements are derived from case studies in publishing CSO Ireland, Eurostat, and World Bank datasets. The contributions will identify required components, missing tools, and highlight best practices to create a statistical Linked Dataspace:</p>

                    <dl>
                        <dt>Required components</dt>
                        <dd>Identification of required components to create a dataspace.</dd>
                        <dt>GraphPusher</dt>
                        <dd>A tool to automate the process of building a dataspace through dataspace descriptions.</dd>
                        <dt>Linked Data Pages</dt>
                        <dd>A publishing framework that allows custom query results and HTML templates for resources.</dd>
                        <dt>Best practices</dt>
                        <dd>Summary of the lessons learned from development pitfalls, workarounds, and best practices on data retrieval, modeling, integration to publishing.</dd>
                    </dl>

                    <h3 id="outline">Thesis Outline</h3>




                    <h2 id="background">Background</h2>
                    <h3 id="dataspaces">Dataspaces</h3>
                    <p>The services that are typically offered in today's information managements systems have to deal with integration and administration of diverse data sources. An abstraction layer for the applications in these systems are considered as <q>Dataspaces</q> as proposed in <a href="http://dl.acm.org/citation.cfm?id=1107502"><cite>From Databases to Dataspaces</cite></a> article. What constitutes a dataspace is that, it would typically consist of a set of data sources with some relationships between them.</p>

                    <dl>
                        <dt>Participants and Relationships</dt>
                        <dd>The participating data sources in a dataspace can be databases, repositories, web services or software packages. The source data can be structured, semi-structured or unstructured, where some may be set to allow updates, yet others only for reading. Similarly, a dataspace may have different relationship models between its sources, with descriptions at any level of granularity, as well as information on data provenance.</dd>

                        <dt>Catalog and Browse</dt>
                        <dd>Dataspaces may include services to allow different modes of interaction with the data sources. Systems might simply provide services to support access to its catalogue in order to allow users to browse its inventory of data resources. Catalogues may have metadata about the resources such as their original source, used schemas, statistics on the data, creators, various timestamps, licensing, completeness and so forth. This type of provenance data provides a perspective for the users and administrators about the elements in the data sources, as well as assistance in reproduction and quality analysis.</dd>

                        <dt>Search and Query</dt>
                        <dd>One other type of service is meant for discovering data by way of searching and querying. The primary function for these services is to allow users to locate and extract particular information from the data sources. A search service offers relevant results to users based on keyword searches, as well as further exploration of the results. Its purpose is to provide a mechanism to deal with large collections of unfamiliar data using natural languages that users are familiar with. A query service in contrast, is meant to provide a structured way to retrieve or manipulate information in participating data sources. One important differentiating factor between searching and querying is that, a query service can let users formulate far more complex questions about the data and get answers to them.</dd>

                        <dt>Local storage and index</dt>
                        <dd>In order to give the data sources a home and to allow inquiry services, a storage and an accompanying index component is used. Local storages and indexes aid in creating efficient queries, precise access to data, and support for data recovery and availability. Indexes are invaluable in terms of identify information across data sources whether locally or references to objects in the real-world.</dd>

                        <dt>Discovery and extensions</dt>
                        <dd>Another type of dataspace service is the discovery component which is used for relating dataspace participants and consequently allow the system to provide better query results. This component would discover, identify and classify data sources and their content in order to easily locate and refer to items in the dataspace in the future. It is important for this component to monitor and allow an environment to update the schema mappings over time in order to accurately represent the dataspace's assets.</dd>

                        <dt>Pay-as-you-go</dt>
                        <dd>In creating semantic relationships between data sources, the involvement of users is usually focused on taking care of most beneficial efforts first. The system is expected to allow increment improvements based on the knowledge of the underlying data's structure, semantics and relationships between sources. Hence, a <em>pay-as-you-go</em> approach to data integration is employed in dataspaces as complete upfront integration is considered to be difficult and is not a required goal.</dd>
                    </dl>


                    <h3>Linked Data</h3>
                    <p>One manifestation of the Semantic Web vision is Linked Data. It is the a pragmatic approach to publishing structured data on the Web in order to discover related data from different sources. The original design principles was put forward by <a about="http://csarven.ca/#i" rel="foaf:knows" href="http://www.w3.org/People/Berners-Lee/card#i">Sir Tim Berners-Lee</a> in 2006 as follows:</p>

                    <blockquote about="http://www.w3.org/People/Berners-Lee/card#i" rel="dcterms:creator" href="http://www.w3.org/DesignIssues/LinkedData.html">
                        <ol>
                            <li>Use URIs as names for things</li>
                            <li>Use HTTP URIs so that people can look up those names.</li>
                            <li>When someone looks up a URI, provide useful information, using the standards (RDF*, SPARQL)</li>
                            <li>Include links to other URIs. so that they can discover more things.</li>
                        </ol>
                    </blockquote>

                    <dl>
                        <dt>HTTP URIs</dt>
                        <dd>In a nutshell, the use of URIs allows us to refer to things and concepts, whether they are real or imaginary, in an absolute way. In order to persistently make use of things, <cite>Sir Tim Berners-Lee</cite> proposed that <q href="http://www.w3.org/DesignIssues/Axioms.html#Universality2">any resource of significance should be given a URI</q>. By employing the widely adopted <code>http:</code> URI scheme, the idea for Linked Data sets off in terms of providing a representation for requested resources.</dd>

                        <dt>RDF data model</dt>
                        <dd>The key ingredient in the information that is returned to the user has to do with the model of the data in the response. Regardless of the syntax that is used, Resource Description Framework (RDF) is essentially an entity-relationship model that provides a way to make statements about the things in our reality. A statement contains three atomic parts, also known as a triple: the <em>subject</em> resource in which the statement is about, followed with a <em>property</em> which is a vocabulary term that describes the type of relationship it has to an <em>object</em> resource. Each of these components are typically represented using HTTP URIs, with the possibility of the object resource being a literal string. In mathematical terms, RDF is a directed, labeled graph, which conceptually depicts a graph of things. What makes this method to make claims about things worthwhile is the act of linking any two URIs together in a particular way. It fundamentally presents an opportunity to discover new resources in an uniform way, whether the resource is in local storage or somewhere else.</dd>

                        <dt>RDF vocabularies</dt>
                        <dd>In RDF triple statements, properties are vocabulary terms that are used to relate a subject to an object. As these resources are accessible via HTTP URIs, when dereferenced they provide a description for the term in use. Some of the well-known vocabularies that are used in Linked Data publishing include: Friend of a Friend (FOAF) to describe people and the things that they do; RDF Data Cube which is used to describe multi-dimensional statistical data; SDMX for the statistical information model; British reference periods, SKOS to describe controlled thesauri, classification schemes and taxonomies; DC Terms for general purpose metadata relations and; VoID to provide metadata on datasets.</dd>

                        <dt>SPARQL</dt>
                        <dd>SPARQL Protocol and RDF Query Language (SPARQL) is a protocol and a query language to retrieve and manipulate RDF data. It can be used to express queries across local and remote data sources, whether the data resides in RDF files or databases. SPARQL queries consist of RDF triple graph patterns written in a fashion similar to Turtle, and allows modifiers for the patterns. In the Linked Data scene, it is common to see publicly accessible SPARQL endpoints where queries are sent and received over HTTP. Federated queries can be written to compute results that span over different SPARQL endpoints on the Web.</dd>
                    </dl>

                    <p>The Linked Data efforts are concerned with publishing and querying all sorts of data that's interconnected in the form of a <em>Giant Global Graph</em>. Some of the motivations behind this is to uncover insights about societies, build smarter systems, making predictions, democratizing data for people, or to make better decisions.</p>


                    <h3 id="linked-statistics">Linked Statistics</h3>
                    <p>With the rise of special-purpose, domain-specific formats such as <a href="http://www.scb.se/Pages/List____314011.aspx">PC-Axis</a> [<a href="#r_1">1</a>] or <a href="http://sdmx.org/">Statistical Data and Metadata eXchange</a> (SDMX) [<a href="#r_2">2</a>], an ISO standard for exchanging and sharing statistical data and metadata among organizations, re-using statistical data has become more possible. However, with the complexity introduced by these formats, the barrier for consuming the data has raised as well. On the other hand, general-purpose formats such as Microsoft's Excel or CSV are very widely deployed and a number of tools and libraries in any kind of programming language one could possibly think of exist to process them. The down-side of these formats is equally obvious: as much of the high-quality annotations and metadata, that is, how to interpret the observations, is not or only partially captured, the data fidelity suffers. Even worse, using these formats, the data and metadata typically gets separated. With linked statistics, one can leverage the existing infrastructure as well as retaining metadata along with the data, yielding high data fidelity, consumable in a standardised, straight-forward way. However, the handling of statistical data as Linked Data requires particular attention in order to maintain its integrity and fidelity.</p>

					<p>Going beyond the operations of slicing, filtering and visualising statistical data typically requires out-of-band information to combine it with other kinds of data. Contextual information is usually not found in the statistical data itself. Using linked statistics, we are able to perform this data integration task in a more straight-forward way by leveraging the contextual information provided by the typed links between the data items of one data set to other datasets in the LOD cloud.</p>

                    <p>The <a href="http://www.w3.org/TR/vocab-data-cube/">RDF Data Cube vocabulary</a> is used to express multi-dimensional statistical data on the Web, and its data model is compatible with the cube model that underlies SDMX. Data cubes are in the nature of a hyper-cube such that multiple dimensions may be used to refer to a particular observation in the cube. Data cubes are characterised by their <em>dimensions</em>, which indicate what the observation is about with a set of properties; its <em>measures</em> to represent the phenomenon that is being observed with a value; and optionally with <em>attributes</em> which help interpret the measure values with a unit. Dimensions typically represent concepts, which are taken from a code list, and are highly valuable as they may used across data cubes by any consumer. Code lists, also known as classifications, are typically identified by using the <em>Simple Knowledge Organization System</em> (SKOS) vocabulary in RDF.</p>

					<p>What linked statistics provide, and in fact enable, are queries across datasets: given the dimensions are linked, one can learn from a certain observation's dimension value, other provided dimension values, enabling the automation of cross-dataset queries, hence cutting down integration costs and delivering results quicker.</p>

					<p>Organisations that are involved in publishing statistical Linked Data and establishing related methodologies and best practices include the <a href="http://data.gov.uk/">UK Government</a> [<a href="#r_3">3</a>], the National Institute of Statistics and Economic Studies (<dfn><abbr title="National Institute of Statistics and Economic Studies">INSEE</abbr></dfn>, France), the U.S. Bureau of Labour Statistics (<dfn><abbr title="Bureau of Labour Statistics">BLS</abbr></dfn>), and the European Environment Agency (<dfn><abbr title="European Environment Agency">EEA</abbr></dfn>). Statistics from many other sources are currently published not by the original statistics producer, but by third parties (universities, web technology companies etc.): U.S. Census 2000, Spanish Census, including historical microdata, EU election results, <a href="http://img.org/">International Monetary Fund</a> (<dfn><abbr title="International Monetary Fund">IMF</abbr></dfn>) commodity prices to name a few as well as the data from Central Statistics Office Ireland, Eurostat, and World Bank, which we will focus on in this paper.</p>


                    <h3>Lifecycles</h3>
                    <p>Summarize all of the lifecycles: http://www.w3.org/2011/gld/wiki/GLD_Life_cycle</p>
                    <p>Are they generic or government or statistical</p>
                    <ul>
                        <li>Hyland et al.</li>
                        <li>Hausenblas et al. http://linked-data-life-cycles.info/</li>
                        <li>Villazon-Terrazas et al.</li>
                        <li>DataLift vision</li>
                        <li>Linked Open Data Lifecycle (LOD2)</li>
                    </ul>


                    <h2 id="statistical-linked-dataspace">Statistical Linked Dataspace</h2>
                    <p>The idea of <em>Dataspaces</em> as discussed earlier was proposed without being tied to any particular set of technologies or types of data. It is a broad description for what constitutes a dataspace. For the moment, without diving into specific technologies, tooling, or data, possible parallels can be drawn between <em>Dataspaces</em> and dataspaces which follows the Linked Data design principles. Table [<a href="#table_dataspaces-to-linked-dataspaces">1</a>] presents a generalized view for the components and services in Linked Dataspaces.</p>

                    <table id="table_dataspaces-to-linked-dataspaces">
                        <caption><strong>Table 1.</strong> Dataspaces to Linked Dataspaces</caption>
                        <thead>
                            <tr><th>Dataspaces</th><th>Linked Dataspaces</th></tr>
                        </thead>
                        <tbody>
                            <tr><td>Catalog</td><td>RDF dataset descriptions</td></tr>
                            <tr><td>Browse</td><td>HTML, RDF publishing framework</td></tr>
                            <tr><td>Local store and index</td><td>RDF files, stores, HTTP URIs</td></tr>
                            <tr><td>Search and Query</td><td>Free-text search and SPARQL</td></tr>
                            <tr><td>Discovery and relationships</td><td>Link discovery framework</td></tr>
                            <tr><td>Data management extensions</td><td>Semi-automatic data deployment</td></tr>
                            <tr><td>Metadata and provenance</td><td>RDF metadata vocabularies</td></tr>
                        </tbody>
                    </table>

                    <p>As Dataspaces are not attached to any specific list of data deployment services or practices, the Linked Dataspace can be seen as a narrower or a particular realization of a Dataspace. Having a dataspace that adheres to Linked Data principles is that, one of the forefront goals is to offer the data sources and some of its services in a way that the underlying data can be globally accessed and linked to by external data sources and applications.</p>

                    <p>The proposal here is that, with the assumption of publishing statistical data in a way that its components can be identified, discovered, and disseminated for numerous uses, a Linked Dataspace can be an ideal candidate. A statistical Linked Dataspace needs to identify resources and provide access to their descriptions, where such resources are in the nature of code lists, data cube observations, datasets and structures. Some of the specific challenges include: extraction of original data sources, transformations to RDF, and loading (ETL) to data storage services; creating global identifiers for the resources in its data catalogs; using vocabularies which are designed for modeling statistical data; offering services to the outside world such that its participating data sources can be browsed through; discovery via text searches and structured queries; and building interlinks with other data sources.</p>


                    <figure id="figure_linked-dataspaces">
                        <figcaption><strong>Figure 9</strong>: Linked Dataspaces architecture</figcaption>
                        <object type="image/svg+xml" data="linked-dataspaces.svg" width="420" height="280"></object>
                    </figure>

                    <h2 id="requirements">Requirements</h2>
<!--
                    <p>Analysis of the original datasets (CSO Ireland, Eurostat, World Bank): What's available, why and what we are interested in (, what we need to prepare to access them?)</p>
                    <h3>Quantity: Huge amount of observations.. characteristics of the datasets (codelists.. in order to interpret observations).. how frequently are they updated (lower-upper bound)? Quality issues.. Provenance.. Not technically. Understanding / Analysis of the datasets.. How are they available (dumps, APIs)</h3>
-->
                    <p>A set of requirements to create a statistical Linked Dataspace are derived from the case studies of deploying CSO Ireland, Eurostat, and World Bank Linked Data. The requirements will be based on analysis of the original data sources, such as their characteristics in terms of quality, quantity, and update frequency.</p>

                    <p>A high-level inspection of published statistical data reveals itself as a collection of significant amounts of data, that's compiled over time by different parties. Naturally the quality of the data sources vary from one publisher to the next, as they try to cater to different publishing criteria. For instance, statistical data is often available in different data formats, with varying vocabularies and thesaurus that's specific to publisher's view about the data. In terms of consumption, one particular example is as follows: the usefulness of raw data that is available in CSV format may depend on the availability and quality of the metadata for the terms that are used in the file. The shape in which the data is available, sets the tone for the data consumer. The data consumers need to make decisions based on the amount of work involved to obtain and work with such data in their own space.</p>


                    <p>Therefore, it is reasonable to conclude that a number of decisions need to be made in order to prepare the dataspace for retrieval and reuse. A questionnaire along the following the lines can help determine the initial required actions to take, grouped into areas in concern:</p>
                    <dl id="requirements-questionnaire">
                        <dt>Overview</dt>
                        <dd>Who is the data publisher?</dd>
                        <dd>Which parties was involved in compiling the original data?</dd>
                        <dd>What data is being published?</dd>
                        <dd>What's the importance of that data?</dd>
                        <dd>Who are the people to contact?</dd>
                        <dd>Is the accuracy of the data indicated?</dd>
                        <dd>Are there different versions or variations of the data?</dd>

                        <dt>Retrieval</dt>
                        <dd>Which formats is the data available in?</dd>
                        <dd>How big is the data?</dd>
                        <dd>How frequently is the data updated, if at all?</dd>
                        <dd>How many retrieval requests need to be made?</dd>
                        <dd>Which access points is the data distributed through?</dd>
                        <dd>Are there special access privileges required to retrieve the data?</dd>

                        <dt>Processing</dt>
                        <dd>Is the necessary tooling in place in order to work with the available formats?</dd>
                        <dd>Are the data files syntactically well-formed?</dd>

                        <dt>Publishing</dt>
                        <dd>What's the data license and terms of use?</dd>
                        <dd>Is there sufficient metadata?</dd>
                        <dd>Which languages is the data and metadata available in?</dd>
                        <dd>What granularity is the data in?</dd>
                    </dl>



                    <h3 id="data-sources">Data sources</h3>
                    <h4 id="data-source_cso-ireland">CSO Ireland</h4>
                    <p>The <a href="http://cso.ie/">Central Statistics Office</a> (<dfn><abbr title="Central Statistics Office">CSO</abbr></dfn>) [<a href="#r_4">4</a>] is the official Irish agency responsible for collecting and disseminating statistics about Ireland. The main source of the statistical data for the CSO is the National Census that is scheduled to be held every five years. The data compiled by the CSO serve as a key input for decision-making in the Irish government and it informs its policies and programmes both at national and local levels.</p>

                    <p>The CSO publishes population statistics in several ways, none of which is particularly suited for direct reuse. The data is primarily available through the CSO's website, formatted for the purpose of display. The CSO offers access to raw demographic data in PC-Axis format for expressing multidimensional statistical data. CSO exposes raw data in an interactive data viewer provided by the <a href="http://www.beyond2020.com/">Beyond 20/20 software</a> [<a href="#r_5">5</a>] that allows to browse, sort and plot the data. It offers a way to export the data in XLS and CSV.</p>

                    <h4 id="data-source_eurostat">Eurostat</h4>
                    <p><a href="http://ec.europa.eu/eurostat">Eurostat</a> [<a href="#r_6">6</a>] is the statistical office of European Union with the aim to provide European Union statistical information in a way that can be comparable at European level. Statistical data collection is done by statistical authorities of each Member States. They verify and analyse the data before sending it to Eurostat. Eurostat's role is to consolidate the statistical data they receive from each Member States and ensure that they are comparable. Eurostat actually only provides harmonized statistical data using common statistical language.</p>

                    <p>Eurostat offers access to datasets using the <a href="http://epp.eurostat.ec.europa.eu/NavTree_prod/everybody/BulkDownloadListing">bulk download facility</a> [<a href="#r_7">6</a>]. The datasets are published by Eurostat in three different formats: TSV, DFT and SDMX. This makes it possible for users to import the data into the tool of their choice. A complete list of datasets which are published by Eurostat is made available through table of contents. Although there is no filtration on the different types of statistics provided by Eurostat, the datasets essentially cover statistical information along the following themes: general and regional statistics, economy and finance, population and social conditions, industry, trade and services, agriculture and fisheries, external trade, transport, environment and energy, and science and technology.</p>

                    <h4 id="data-source_world-bank">World Bank</h4>
                    <p>The <a href="http://worldbank.org/">World Bank</a> [<a href="#r_9">9</a>] is an international development organization that provides access to a comprehensive set of data about countries around the globe. The publicly available statistical data is collected from officially-recognized international sources, and consists of a wide array of observations on development indicators, financial statements, climate change, projects and operations.</p>

                    <p>The World Bank provides a free and open access to numerous datasets in their <a href="http://data.worldbank.org/data-catalog">data catalog</a> [<a href="#r_10">10</a>]. These datasets are available in one or more formats: XML, JSON, CSV, XLS; with additional geospatial data in SHP and KML, and supporting documentations in PDF. The World Bank APIs offers some of the datasets primarily in XML and JSON representations, whereas the rest of the formats are available as data dumps. In our use-case, the decision on which datasets to work with was based on several factors such as the importance of the dataset, its completeness, and the ease of converting it into an RDF representation. Hence, the following datasets from the World Bank's API was selected with the preference of working with XML:</p>

                    <ul>
                        <li><a href="http://data.worldbank.org/developers/climate-data-api">World Bank Climate Change</a> (<dfn><abbr title="World Bank Climate Change">WBCC</abbr></dfn>) [<a href="#r_11">11</a>] contains data from historical observations and future projections derived from global circulation models.</li>
                        <li><a href="http://data.worldbank.org/data-catalog/world-development-indicators">World Development Indicators</a> (<dfn><abbr title="World Development Indicators">WDI</abbr></dfn>) [<a href="#r_12">12</a>] contain various global development data on world view, people, the environment, the economy, states and markets, and global links. It includes national, regional and global estimates.</li>
                        <li><a href="https://finances.worldbank.org/">World Bank Finances</a> (<dfn><abbr title="World Bank Finances">WBF</abbr></dfn>) [<a href="#r_13">13</a>] cover Bank's investments, assets it manages on behalf of global funds, and the Bank's own financial statements.</li>
                        <li><a href="http://data.worldbank.org/data-catalog/projects-portfolio">World Bank Projects and Operations</a> (<dfn><abbr title="World Bank Projects and Operations">WBPO</abbr></dfn>) [<a href="#r_14">14</a>] provides information about the lending projects from 1947 to present along with links to publicly disclosed online documents.</li>
                    </ul>


                    <h3 id="data-retrieval">Data Retrieval</h3>
                    <h4 id="data-retrieval_cso-ireland">CSO Ireland</h4>
                    <p>Data from the 2006 Irish Census was retrieved manually, downloading each slice of small area population statistics individually via the export to Excel files from the <a href="http://beyond2020.cso.ie/census/ReportFolders/ReportFolders.aspx">CSO's instance of the Beyond 20/20</a> [<a href="#r_15">15</a>]. 14 datasets in CSV format in total of 8MB was manually retrieved using the interactive application by clicking on the access URLs.</p>

                    <h4 id="data-retrieval_eurostat">Eurostat</h4>
                    <p>There are approximately 6100 datasets published by Eurostat. Eurostat updates information about datasets as well as table of contents twice a day. The datasets holds statistics on daily, monthly, quarterly and annual basis. Therefore, certain datasets are updated daily and many datasets are updated on monthly basis. To keep Eurostat RDF datasets up to date, we've scheduled a cronjob, which runs a set of scripts on weekly basis. In order to avoid unnecessary RDF transformations of each dataset, we've only updated the changed datasets within the past week.</p>

                    <p>Along with the datasets, Eurostat also publishes Data Structure Definitions (<dfn><abbr title="Data Structure Definition">DSD</abbr></dfn>) about each dataset, as well as a set of dictionaries (code lists) which are shared among all datasets. Hence we take into consideration every type of information provided by Eurostat. The DSDs are published in XML format while the code lists are published in TSV format. Dataset is available in three different formats: TSV, DFT and SDMX. Given that the datasets, metadata and code lists each provide different type of information and is represented differently, we wrote Java programs to process XML and TSV formats of the metadata, dataset and code lists separately.</p>

                    <p>We have written different shell scripts which wrap each Java program and one main script which handles the whole process of data downloading and transformation by invoking other scripts.</p>

                    <p>Over 13000 HTTP GET requests was made to Eurostat to download raw datasets, DSDs and code lists, with a total of disk space of ~58GB.</p>

                    <h4 id="data-retrieval_world-bank">World Bank</h4>
                    <p>The World Bank datasets were collected by making requests to the World Bank API endpoints using the XML output format.</p>

                    <p>World Bank APIs was called ~150000 times to retrieve all of the WDI, WBF, WBCC, WBPO datasets, with a total disk space of ~20GB.</p>

                    <p>The data is retrieved at irregular periods - at least once a month - from the World Bank API endpoints. The retrieval act is partly based on new dataset announcements in World Bank mailing lists. Although the data retrieval and transformation phases are conducted by independent scripts, the commitment to retrieve and store the data is based on achieving the quality of the eventual RDF serialization. Therefore, Java and Bash scripts are manually executed to retrieve, in order to closely monitor abnormalities in the responses and account for necessary changes in the transformations.</p>



                    <h2 id="components">Components</h2>
                    <p>Given the requirements to publish a statistical Linked Dataspace, Table 9 outlines a list of required components to realize such space with example implementations.</p>

                    <table id="linked-dataspace-components-implementations">
                        <caption><strong>Table 9.</strong> Linked Dataspace components, and implementations</caption>
                        <thead>
                            <tr><th>Components</th><th>Implementations</th></tr>
                        </thead>
                        <tbody>
                            <tr><td>Environment</td><td>OS: Linux, Mac, Windows; Server: Apache, IIS</td></tr>
                            <tr><td>Data retrieval</td><td>CURL, Java, LDSpider, wget</td></tr>
                            <tr><td>Data preprocessing</td><td>Manual with custom scripts, Google Refine</td></tr>
                            <tr><td>Data modeling</td><td>Google Refine, Manual, Neogolism</td></tr>
                            <tr><td>Data conversion</td><td>XSLT, CSV2RDF, D2R</td></tr>
                            <tr><td>RDF store and SPARQL servers</td><td>TDB, Fuseki, Virtuoso, Sesame, 4store</td></tr>
                            <tr><td>Loading RDF data to RDF store</td><td>Manual with custom scripts</td></tr>
                            <tr><td>RDF store optimization</td><td>TDBstats</td></tr>
                            <tr><td>Interlinking</td><td>Manual, LIMES, Silk</td></tr>
                            <tr><td>Enrichment</td><td>Manual, Stanbol</td></tr>
                            <tr><td>User interface</td><td>Callimachus, Linked Data API, OpenLink Data Explorer, Pubby</td></tr>
                            <tr><td>Database metadata</td><td>VoID, LODStats</td></tr>
                            <tr><td>Data dumps</td><td>File system</td></tr>
                            <tr><td>Applications</td><td>LodLive, SGVizler, Tabulator</td></tr>
                        </tbody>
                    </table>

                    <p>For some of the components, there are several implementations that are publicly available for use. A range of improvements can be done for each implementation with varying complexity. In the case studies, two areas in particular were identified for possible areas for improvements given the state of the art of publicly available technologies: loading of RDF data to RDF store, and a human user interface.</p>
                    <p>It turned out that loading RDF data was a recurring task that had to be performed by the administrator. Hence, the <a href="#graphpusher">GraphPusher</a> tool was built to improve on the manual approach by automating bulk data. GraphPusher was primarily used to help to deploy the Irish Government Linked Dataspace at <a href="http://data-gov.ie/">DataGovIE</a> and this is a report on preliminary findings on using this tool. As discussed in data sources for CSO Ireland, the data primarily consists of statistical data about the Irish population.</p>

                    <p>On a similar note, <a href="#linked-data-pages">Linked Data Pages</a> was written to provide better control over front-end templating system to present a Web interface for RDF data. The Linked Data Pages framework was used to publish CSO Ireland and <a href="http://worldbank.270a.info/">World Bank</a> Linked Dataspaces.</p>

                    <p>Both of these contributions are discussed in further detail in the following sections.</p>

                    <h3 id="graphpusher" class="todo" title="Extend in-context of Statistical Linked Data.">GraphPusher</h3>
                    <p>The manual creation of dataspaces [<a href="#r_1">1</a>] conforms to Linked Data principles is time consuming, and prone to human errors since there are a number of steps involved to gather the data, and place them in a store. Typically a list is compiled consisting of datasets with graph names for each dataset, and local copies of the data that is to be imported in to a store. This information may be tracked in a structured format, or a simple text file. The Vocabulary of Interlinked Datasets (VoID) [<a href="#r_2">2</a>] is used because it is an accepted standard to describe RDF datasets, as well as the SPARQL 1.1 Service Description (SD) [<a href="#r_3">3</a>] for discovering information about a SPARQL service.</p>

                    <p>One of the goals of publishing a VoID file alongside datasets that are published as Linked Data is to allow consumers to discover and retrieve the data. Based on this, the GraphPusher tool [<a href="#r_4">4</a>] is designed to aid users to retrieve and import data in RDF format into a graph store by making use of the metadata in a VoID file. VoID describes access methods for the actual RDF triples in a number of different ways. GraphPusher focuses on two of these methods; dereferencing HTTP URIs and compressed RDF data dumps.</p>

                    <p>Given RDF data that is to be imported into a graph store, additional information is required in order to carry out the update process; how to access the store, dataset to store in, and graph name to use. It naturally follows that this unique information needs to be handed over to the store per dataset for the update. When dataspaces are updated frequently, it brings forward a reason to perform with a script, minimizing human involvement and errors. Therefore, the underlying purpose for GraphPusher is to help with data deployment as efficiently as possible.</p>

                    <p>Where users are publishers and consumers at the same time, the creation of a VoID description can be seen as a declarative programming approach to putting RDF data into data stores. GraphPusher&#8217;s potential use is to help applications to pull in Linked Data.</p>

                    <p>GraphPusher was originally created to fulfil the data deployment need of DataGovIE&#8217;s [<a href="#r_5">5</a>] production and staging datasets. DataGovIE reuses its published VoID description to feed its own dataspace with incremental changes to data.</p>

                    <p>GraphPusher approaches this task with the following sequence. Figure [<a href="#diagram_graphpusher-sequence">1</a>] gives an illustration.</p>

                    <ol>
                        <li>Compiling a list of datasets to be retrieved to local space and graph name to use per dataset or file. This is accomplished by retrieving a VoID file and extracting the <code>void:dataDump</code> property values. It also looks for SD&#8217;s <code>sd:name</code> property in VoID, where they are collected to name the graphs in the RDF store. In the case that <code>sd:name</code> is not present, the graph name method is determined by user&#8217;s configuration in GraphPusher.</li>

                        <li>Datasets are downloaded to local disk. For compressed dataset archives, they are decompressed.</li>

                        <li>Finally, the data is imported into the graph store by either executing an update operation on the RDF store directly, or through the SPARQL service via SPARQL Update.</li>
                    </ol>

                    <figure id="figure_graphpusher-sequence">
                        <figcaption><strong>Figure 1</strong>: GraphPusher sequence</figcaption>
                        <object type="image/svg+xml" data="graphpusher-sequence.svg" width="300" height="220"></object>
                    </figure>

                    <h4 id="graphpusher_implementation">Implementation</h4>
                    <p>A reoccurring dataspace operation in Linked Data environments is the retrieval of remote data and placing them in an RDF storage. In the case of rebuilding an RDF store, the approach is to use a VoID description for the datasets in order to batch process some of the recurring steps. This is typically accomplished by publishing a VoID description of the datasets with triples using the <code>void:dataDump</code> property, and optionally with <code>sd:graph</code> and <code>sd:name</code> properties.</p>

                    <h4 id="graphpusher_related-work">Related work</h4>
                    <p>To the best of publicly available practices, there exists no technology that builds a dataspace automatically. The work is based on VoID as SADDLE [<a href="#r_11">11</a>] and DARQ [<a href="#r_12">12</a>] do not provide the required metadata coverage for dataset access, and VoID has 30% coverage in the Linking Open Data (LOD) [<a href="#r_13">13</a>] and is actively maintained.</p>

                    <h4 id="graphpusher_about-the-code">About the code</h4>
                    <p>GraphPusher takes a VoID URL as input from the command-line, retrieves the VoID file, looks for the <code>void:dataDump</code> property values, HTTP GETs them, and finally imports them into an RDF store using one of the graph name methods. The graph name method is defined as part of GraphPusher&#8217;s configuration.</p>

                    <p>In order to specify the location of the RDF store for the data that is to be imported, GraphPusher can take either TDB Triple Store&#8217;s [<a href="#r_6">6</a>] assembler file which contains configuration for graphs and datasets, or a SPARQL service URI. If the TDB assembler filename is provided in the command-line argument with <code>--assembler</code>, GraphPusher checks this file to determine the name of the dataset and its location. It will then import the data dumps using the TDB loader. Alternatively, with the <code>--dataset</code> option, GraphPusher uses Apache Jena&#8217;s SPARQL over HTTP command-line script [<a href="#r_7">7</a>] to HTTP POST by using the SPARQL 1.1 Graph Store HTTP Protocol [<a href="#r_8">8</a>].</p>

                    <p>GraphPusher is written in Ruby, and relies on the rapper [<a href="#r_9">9</a>] RDF parser utility, and optionally the TDB Triple Store. GraphPusher is configurable, e.g., location to store the data dumps, method to determine named graphs. It decompresses archived data dumps to a local directory, converts files with unrecognized RDF format extensions to Turtle format before importing. It is tested and functional in Debian/Ubuntu systems, available under the Apache License 2.0 [<a href="#r_10">10</a>].</p>

                    <h4 id="graphpusher_determining-graph-name">Determining graph name</h4>
                    <p>Where possible, GraphPusher makes a decision for dataset&#8217;s named graphs by staying consistent with publishers&#8217; intentions. In certain cases, it tries to indicate the origin of the data by reusing the dataset location in the graph name.</p>

                    <p>Figure [<a href="figure_named-graph-flowchart">2</a>] illustrates the flow for this process.</p>

                    <p>If <code>sd:name</code> is present in VoID, its value has highest priority because it explicitly states the named graph that is used in the dataset, and consequently how it can be referenced in a <code>FROM/FROM NAMED</code> clause.</p>

                    <p>If <code>sd:name</code> is not present, GraphPusher looks for one of the <code>graphNameMethod</code> configuration property values: <code>dataset</code>, <code>dataDump</code>, <code>filename</code>. By default <code>dataset</code> value tells the GraphPusher to simply use the URI of the <code>void:Dataset</code>. If <code>dataDump</code> is set, the named graph IRI becomes the IRI of the data dump. Alternatively, if the <code>filename</code> method is used, the file name is appended to the <code>graphNameBase</code> URI value.</p>

                    <figure id="figure_named-graph-flowchart">
                        <figcaption><strong>Figure 2</strong>: Determining named graph flowchart</figcaption>
                        <object type="image/svg+xml" data="graphpusher-named-graph-flowchart.svg" width="360" height="340"></object>
                    </figure>

                    <h4 id="graphpusher_preliminary-results">Preliminary Results</h4>
                    <p>As there exists no automated approach to have local copies of the data dumps, determining a graph name per data dump, and importing the data into a store, the manual approach is used as baseline to compare against.</p>

                    <p>Table [<a href="#table_comparison-manual-graphpusher-approaches">1</a>] gives an overview for the comparison of manual and GraphPusher approaches.</p>

                    <table id="table_comparison-manual-graphpusher-approaches">
                        <caption><strong>Table 1</strong>: Comparison of manual and GraphPusher approaches</caption>
                        <thead>
                            <tr><td></td><th>Manual</th><th>GraphPusher</th></tr>
                        </thead>
                        <tfoot>
                            <tr>
                                <td colspan="3">
                                    <dl>
                                        <dt>Configurability</dt>
                                        <dd>Used to indicate the flexibility of the approaches. The manual approach allows the user to practically to carry out the loading process as they see fit, whereas the GraphPusher approach comes with a predetermined list of configuration options to import the data. In that sense, GraphPusher is naturally limited to its feature set.</dd>
                                        <dt>Requirements</dt>
                                        <dd>Provided that both approaches use the same set of tools behind the scenes, this criteria is meant to highlight the main difference between them.</dd>
                                        <dt>Efficiency</dt>
                                        <dd>This criteria outlines the amount of work required by the user for recurring tasks. The manual approach undoubtedly required the user to be fully involved in contrast to GraphPusher.</dd>
                                    </dl>
                                </td>
                            </tr>
                        </tfoot>
                        <tbody>
                            <tr><th>Configurable</th><td>Full</td><td>Predefined</td></tr>
                            <tr><th>Requirements</th><td>None</td><td>VoID</td></tr>
                            <tr><th>Efficiency</th><td colspan="2">See discussion below</td></tr>
                        </tbody>
                    </table>

                    <p>The bare-bone manual approach would typically make use of <em>wget</em> or <em>curl</em> to download the dataset files, several archive tools to decompress compressed datasets, and an RDF parser to convert a given dataset file in an RDF format to N-Triples format, where it is suitable for pattern matching with the <em>grep</em> tool.</p>

                    <p><strong>Efficiency</strong>: The steps mentioned above would require the user to posses knowledge of command-line operations and running SPARQL queries, and depending on their expertise and available tools, given test runs, it takes 20 minutes or more if executed by hand. The automated approach in GraphPusher would typically take a minute or two to configure and run from command-line. The efficiency of each approach is noteworthy, especially when this task is repeated for different datasets and metadata.</p>

                    <p>It should be noted here that, the expertise and time needed to have the required tools set up in the system is excluded as they vary from one favourable approach to another in a given operating environment. VoID is GraphPusher&#8217;s primary requirement, hence the assumption is that a VoID file exists, or can be generated.</p>

                    <h4 id="graphpusher_conclusions">Conclusions</h4>
                    <p>GraphPusher illustrates a direct application of reusing a VoID description to pull in datasets to a dataspace. At its core, it takes advantage of RDF&#8217;s <em>follow your nose</em> discovery to support data digestion pipelines. This is in line with the Linked Data methodology as it mostly relies on dereferenceable HTTP URIs.</p>

                    <p>Given the following limitations of GraphPusher, there is future work in plans:</p>
                    <ul>
                        <li>Retrieval of a VoID description in an RDF store via a SPARQL Endpoint.</li>
                        <li>Ability to skip data dump retrievals by checking for datasets&#8217; timestamps in HTTP headers, and using local copies where appropriate.</li>
                        <li>Techniques to optimize storage footprint for duplicate copies of retrieved datasets.</li>
                        <li>Merging all dump files per graph into a single N-Triples file before importing to the RDF store for more efficient loading. Consequently, this approach needs to account for blank nodes which occur in different files with the same identifier.</li>
                        <li>Support other popular RDF stores e.g., Virtuoso RDF Quad Store [<a href="#r_14">14</a>] and 4store [<a href="#r_15">15</a>], to directly communicate with the store &#8211; currently TDB Triple Store is supported.</li>
                    </ul>

                    <p>Based on the corrections which were made on the conversion scripts for the data model, the RDF store had to be loaded several times. In the earlier stages of Fuseki and its TDB RDF store, the store also had to be rebuilt in order to optimize query responses accordingly. Since the size of the 2006 Irish Census RDF data dumps are under 15MB compressed in total, it was a negligible step to download them to a temporary location in order to bulk load. Given the current limitation of GraphPusher not being able to work with local files, it wasn't suitable to download the compressed RDF data dumps of World Bank data, weighing around 400MB, repeatedly, during the course of its development.</p>


                    <h3 id="linked-data-pages" class="todo" title="Add a screenshot of an HTML view">Linked Data Pages</h3>
                    <p>Linked Data Pages (LDP) is a Web publishing framework to generate RDF and HTML documents. It invokes unique SPARQL queries based on requested URIs and outputs the results to corresponding HTML templates. Links to alternate RDF formats as well as JSON are handled by content-negotiation.</p>

                    <h4>Motivation</h4>
                    <p>Initial reasons to develop LDP in 2010 was to move beyond the de facto generic entity rendering for RDF resources That is, the requested URI corresponded with the result of <code>DESCRIBE</code> SPARQL query, where the property and object resources were primarily displayed in a key-value fashion. </p>

                    <p>While allowing default page rendering as base, if the developer had knowledge about the resources or the data model, it was reasonable to provide a way for them to uniquely display the resources to be more human friendly.</p>

                    <h4>Overview</h4>
                    <p>It is built on top of <a href="http://code.google.com/p/moriarty/">Moriarty</a>, <a href="http://code.google.com/p/paget/">Paget</a>, and <a href="https://github.com/semsol/arc2">ARC2</a> libraries. It uses Paget to dispatch URIs and build a local index from the SPARQL query result.</p>

                    <p>Describe the necessary steps for the entity. Compare to other approaches (Sig.ma). See ODE for different types of entities. Perhaps even from other fields. Heterogeneous data sources in a unified view.</p>


                    <h4 id="linked-data-pages_installation">Installation</h4>
                    <p>There is an installation web page to get LDP going. The same set of configuration settings in the script can be entered directly in the configuration file. The configuration is fairly basic and primarily consists of the following:</p>

                    <dl>
                        <dt>Directories</dt>
                        <dd>Directory paths where each of the libraries (LDP, Paget, Moriarty and ARC2) reside in the file system.</dd>
                        <dt>SPARQL service URI</dt>
                        <dd>This is typically the URI that the SPARQL service accepts queries under.</dd>
                    </dl>

                    <h4 id="linked-data-pages_site-configuration">Site configuration</h4>
                    <p>The Web service can be configured with the following parameters:</p>

                    <dl>
                        <dt>Site settings</dt>
                        <dd>Site name and domain name, root URI for access, theme to use, and site logo.</dd>
                        <dt>URI host mapping</dt>
                        <dd>If the host name that the service is running under is different from the host name that is used in the URIs in the RDF data, this setting allows mapping between the the two. For example, if there is a request is made to <code>http://example.org/foo</code>, the setting can be set to look for <code>http://example.net/foo</code> in the RDF store instead.</dd>
                    </dl>

                    <p>LDP is bundled with a default CSS theme, but allows different stylesheets to be used by simply providing its location.</p>

                    <p>The reason to develop the URI host mapping feature was based on the fact that in the early stages of modeling 2006 Census data, the URI patterns was designed based on the knowledge of having the data work under <code>govdata.ie</code>. During the staging phase of publishing the RDF data under <code>data-gov.ie</code>, the URI patterns were not changed until further notice.</p>


                    <h4 id="linked-data-pages_resource-configuration">Resource configuration</h4>
                    <p>LDP allows prefixes to be assigned for the URIs in the configuration file. It is put to use when working with query response values inside templates.</p>

                    <p>LDP provides a way to organize what the URIs should describe and how they should be presented for the user in HTML and RDF. This is accomplished by putting three components together. The first is the URI pattern in question that is being offered, the second is the SPARQL query that it triggers when requested, and third is the template that is to be used to render the HTML page. Definitions of each of the components are defined as such:</p>

                    <dl>
                        <dt>URI pattern</dt>
                        <dd>Patterns are triggered in a way that when a URI is recognized, the most precise URI is selected for the response. This is in contrast to more relaxed patterns which are used to display. For instance, for code lists, each concept URI can be rendered the same way, such that it makes sense catch all concepts at their root path (code list) as opposed their full path. e.g., When <code>http://worldbank.270a.info/classification/country/CA</code> is requested, there would be a URI pattern looking out for <code>http://worldbank.270a.info/classification/country/</code> to respond with an output for all countries the same way. Of course, if one wishes to render <code>http://worldbank.270a.info/classification/country/CA</code> differently than the other countries, it can do so by using that URI pattern in the configuration.</dd>
                        <dt>SPARQL Queries</dt>
                        <dd>Once the URI pattern is determined, a SPARQL query needs to be triggered to retrieve a description of the resource. A full blown SPARQL query pattern can be entered here for each resource. If there is no URI pattern matched, it will default to <code>DESCRIBE &lt;URI&gt;</code>.</dd>
                        <dt>HTML templates</dt>
                        <dd>When an URI pattern is matched, along with the SPARQL query to trigger, an HTML template is assigned at the same time. The template has access to the response gathered from the SPARQL query. An index of values can be either be accessed directly, or a set of helper methods can be used directly from the template. For example,the <code>getTriples</code> function is used to find triples in an index (a multi-dimensional array). When no parameters are provided, it will simply return the full response. It can be used to look for a particular triple in the response. <code>$triples = $this->getTriples('http://csarven.ca/#i');</code> can be used directly in the template to get all of the triples with subject <code>http://csarven.ca/#i</code>. Similarly, <code>$this->getTriples('http://csarven.ca/#i', 'http://xmlns.com/foaf/0.1/knows');</code> can be used to get triples which match the pattern with given subject and property URIs. An alias to this is the <code>getValue</code> function where qnames can be used in the property position: <code>$this->getValue('http://csarven.ca/#i', 'foaf:knows');</code>. For more complex templating, for more data processing with PHP, the <code>SITE_Template</code> class offers more helper functions and can be extended.</dd>

                        <dd>If there is no URI pattern matched, the default template will be used to output the results. Given that the SPARQL query will use <code>DESCRIBE</code>, the default rendering of this template is displayed in a key-value pair fashion.</dd>
                    </dl>

                    <p>This resource configuration approach provides a flexible way for site owners to prepare responses for URIs as well as unique user interfaces.</p>


                    <h4 id="linked-data-pages_conclusions">Conclusions</h4>
<p class="todo">Differentiate Conceptual presentation vs. implementation ??????</p>

                    <p>The LDP PHP framework is relies on Paget, Moriarty, and ARC2 as they take care of the lower level work. Hence, LDP functions by the fact that all these components are in a sense <em>stitched</em> together.</p>

                    <p>From the implementation side, Linked Data Pages provides results based on the URI pattern that is triggered. In other words, what information a resource contains and how it is represented is determined at the level of the URI itself. An alternative approach to outputting an HTML document would be based on the characteristics of the resource at hand. That is, the triple graph pattern from a SPARQL query result would allow the developer to shift through the results and assign a template in context of the triples at hand.</p>

                    <p>LDP is capable of taking care of the bulk of the work that's required to publish Linked Data. The <a href="http://csarven.ca/how-to-create-a-linked-data-site">how to create a Linked Data site</a> explains the full process in further detail, from setting up a SPARQL service, importing data, to publishing pages using LDP.</p>





                    <h2>Implementation of the Case Studies</h2>
                    <p>Describe from the deployment side.. Why TDB, LIMES..</p>
                    <p>Explain how I've built these spaces</p>

                    <h3>Deployment Architectures</h3>

                    <h3>Data Retrieval and Preprocessing</h3>
                    <h4>Java / Linux shell</h4>



                    <h3>Data Modeling</h3>
                    <h4>Vocabularies</h4>
                    <h4>URL Patterns</h4>
                    <h4>Data Structure Definitions</h4>

                    <h3>Interlinking</h3>
                    <h4>Manual</h4>
                    <h4>LIMES (requires curation)</h4>

                    <h3>Enrichment</h3>
                    <h3>Provenance</h3>


                    <h3>Data Conversion</h3>
                    <h4>Java / Python / saxonb-xslt (command-line; performance..)</h4>


                    <h3>Building RDF stores</h3>
                    <h4>Linux shell</h4>
                    <h4>GraphPusher</h4>


                    <h3>SPARQL Endpoint</h3>
                    <p>Public, Performance (optimization with TDB)</p>


                    <h2>Publication</h2>
                    <h3>VoID+SD</h3>
                    <ul>
                        <li>Dataset Metadata</li>
                        <li>Data Statistics (LODStats)</li>
                    </ul>

                    <h3>User Interface</h3>
                    <h4>Linked Data Pages</h4>


                    <h3>Applications</h3>
                    <h4>Building simple visualizations</h4>



                    <h3>Data dumps</h3>
                    <h3>License</h3>
                    <h3>Announcing the Datasets</h3>
                    <h3>Source Code</h3>





                    <h2>Conclusions</h2>

                    <div id="references">
                        <h2>References</h2>
                        <ol about="[this:]">
<!--
<li id="r_2">Ngonga Ngomo, A.-C.: <em>A Time-Efficient Hybrid Approach to Link Discovery</em>, The Sixth International Workshop on Ontology Matching, ISWC (2011), <a rel="dcterms:references" href="http://www.dit.unitn.it/~p2p/OM-2011/om2011_Tpaper1.pdf">http://www.dit.unitn.it/~p2p/OM-2011/om2011_Tpaper1.pdf">http://www.dit.unitn.it/~p2p/OM-2011/om2011_Tpaper1.pdf</a></li>
-->
                        </ol>
                    </div>
                </div>
            </div>
        </div>
    </body>
</html>
