<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML+RDFa 1.0//EN" "http://www.w3.org/MarkUp/DTD/xhtml-rdfa-1.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"
    xmlns:xsd="http://www.w3.org/2001/XMLSchema#"
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
    xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
    xmlns:owl="http://www.w3.org/2002/07/owl#"
    xmlns:rsa="http://www.w3.org/ns/auth/rsa#"
    xmlns:cert="http://www.w3.org/ns/auth/cert#"
    xmlns:dcterms="http://purl.org/dc/terms/"
    xmlns:foaf="http://xmlns.com/foaf/0.1/"
    xmlns:v="http://www.w3.org/2006/vcard/ns#"
    xmlns:cc="http://creativecommons.org/ns#"
    xmlns:dbr="http://dbpedia.org/resource/"
    xmlns:dbp="http://dbpedia.org/property/"
    xmlns:sioc="http://rdfs.org/sioc/ns#"
    xmlns:wgs="http://www.w3.org/2003/01/geo/wgs84_pos#"
    xmlns:cal="http://www.w3.org/2002/12/cal/ical#"
    xmlns:org="http://www.w3.org/ns/org#"
    xmlns:biblio="http://purl.org/net/biblio#"
    xmlns:book="http://purl.org/NET/book/vocab#"
    xmlns:ov="http://open.vocab.org/terms/"
    xmlns:this="http://csarven.ca/statistical-linked-dataspace"
    xml:lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
        <title>Statistical Linked Dataspaces</title>
        <meta name="description" content=""/>
        <link rel="stylesheet" type="text/css" media="all" href="display.css"/>
    </head>

    <body about="[this:]" typeof="foaf:Document sioc:Post biblio:Paper" class="hfeed journal">
        <div id="wrap">
            <div class="hentry">
                <h1 property="dcterms:title" class="entry-title">On the design of statistical Linked Dataspaces</h1>

                <div id="authors">
                    <dl>
                        <dt>Authors</dt>
                        <dd class="entry-author"><a about="[this:]" rel="dcterms:creator dcterms:publisher dcterms:contributor" href="http://csarven.ca/#i">Sarven Capadisli</a><sup><a href="#author_email_1">1</a></sup> <sup><a href="#author_org_a">a</a></sup></dd>
                    </dl>

                    <ul id="author_emails">
                        <li id="author_email_1"><sup>1</sup><a about="http://csarven.ca/#i" rel="foaf:mbox" href="mailto:info@csarven.ca" class="author_email">info@csarven.ca</a></li>
                    </ul>

                    <ul id="author_orgs">
                        <li id="author_org_a"><sup>a</sup><span about="http://csarven.ca/#i" rel="org:memberOf" resource="http://deri.ie/"/><a href="http://deri.ie/">Digital Enterprise Research Institute</a>, <a href="http://nuigalway.ie/">NUI, Galway</a>, Ireland</li>
                    </ul>
                </div>

                <div id="abstract" class="entry-summary">
                    <h2>Abstract</h2>
                    <p property="dcterms:abstract" datatype="">~The story here is ..</p>
                </div>

                <div property="dcterms:description" id="content" class="entry-content">
                    <div id="document-identifier">
                        <h2>ID</h2>
                        <p><code>http://csarven.ca/statistical-linked-dataspace</code></p>
                    </div>

                    <div id="keywords">
                        <h2>Keywords</h2>
                        <ul about="[this:]" rel="dcterms:subject">
                            <li><a resource="http://dbpedia.org/resource/Linked_Data" href="http://en.wikipedia.org/wiki/Linked_Data">Linked Data</a></li>
                            <li><a resource="http://dbpedia.org/resource/Data_modeling" href="http://en.wikipedia.org/wiki/Data_modeling">Data modeling</a></li>
                            <li><a resource="http://dbpedia.org/resource/Dataspaces" href="http://en.wikipedia.org/wiki/Dataspaces">Dataspaces</a></li>
                            <li><a resource="http://dbpedia.org/resource/Knowledge_management" href="http://en.wikipedia.org/wiki/Knowledge_management">Knowledge management</a></li>
                            <li><a resource="http://dbpedia.org/resource/Statistics" href="http://en.wikipedia.org/wiki/Statistics">Statistics</a></li>
                            <li><a resource="http://dbpedia.org/resource/Life_cycle" href="http://en.wikipedia.org/wiki/Life_cycle">Life cycle</a></li>
                        </ul>
                    </div>

                    <h2>Acknowledgements</h2>

                    <h2>Contents</h2>
                    <ul>
                        <li>...</li>
                    </ul>


                    <h2 id="introduction">Introduction</h2>
                    <h3 id="problem-statement">Problem statement</h3>
                    <p>Convenient management of dataspaces for statistical Linked Data remains to be costly and lacks integrated and automated tool support. The design considerations and requirements are therefore at the forefront for reasonable functionality of these dataspace platforms.</p>

                    <p>Access to statistical data, primarily in the public sector has exploded in recent years. While these initiatives provide new opportunities to get insights on societies, management of the dataspaces are consequently confronted with new challenges. As centralized dataspaces are now faced with more heterogeneous data collections, with varying quality, solutions which employ the Linked Data principles appear to be promising.</p>

                    <p>However, due to a range of technical challenges, development teams often face low-level repetitive data management tasks with partial tooling at their disposal. These challenges on the surface include: addressing data integration, synchronisation, and access. Within the context of statistical data, the expectations from these dataspaces is that, a Linked Data tool-chain is utilized, and the data is accessible to both humans as well as the machines.</p>


                    <h3 id="hypothesis">Hypothesis</h3>
                    <p>It is contended that the deployment of statistical Linked Data comes with a specific set of requirements. The degree in which the requirements are accomplished predetermines the dataspace's usefulness for data consumers. Therefore, the hypothesis of this document is if a specific set of requirements for building and managing dataspaces for statistical Linked Data is possible, and the development of missing parts to do some of the processes better by reducing cost and making the dataspace more useful.</p>

                    <h3 id="contributions">Contributions</h3>
                    <p>With preference to minimizing developer intervention wherever possible, the contributions herein are within the expectations of working in Linked Dataspace and providing read access to data respectively. The set of design considerations and requirements are derived from case studies in publishing CSO Ireland, Eurostat, and World Bank datasets. The contributions will identify required components, missing tools, and highlight best practices to create a statistical Linked Dataspace:</p>

                    <dl>
                        <dt>Required components</dt>
                        <dd>Identification of required components to create a dataspace.</dd>
                        <dt>GraphPusher</dt>
                        <dd>A tool to automate the process of building a dataspace through dataspace descriptions.</dd>
                        <dt>Linked Data Pages</dt>
                        <dd>A publishing framework that allows custom query results and HTML templates for resources.</dd>
                        <dt>Best practices</dt>
                        <dd>Summary of the lessons learned from development pitfalls, workarounds, and best practices on data retrieval, modeling, integration to publishing.</dd>
                    </dl>

                    <h3 id="outline">Thesis Outline</h3>
                    <p>Chapter 2 presents <a href="#background">background</a> material for this document. It covers the essentials of the dataspace concept and summarizes its core components. The Linked Data design principles as well as Linked Statistics are explained in order to draw a bridge from dataspaces. Existing Linked Lifecycle models are also summarized in order to provide the basis of the problem space. Chapter 3 contains an overview of the Statistical Linked Dataspace, the core proposal of this document. It makes a connection between the general dataspaces concept with Linked Dataspaces, and proposes that Linked Dataspaces is well suited for deploying statistical data by outlining its requirements. Chapter 4 then outlines the required components with actual implementations of the technologies where GraphPusher and Linked Data Pages are put forward as contributions. Chapter 5 ties it all together by examining the phases and components of the case studies to realize Statistical Linked Dataspaces. Chapter 6 concludes by analysing the work described, results achieved, lessons learned, and the feature work up ahead.</p>



                    <h2 id="background">Background</h2>
                    <h3 id="dataspaces">Dataspaces</h3>
                    <p>The services that are typically offered in today's information managements systems have to deal with integration and administration of diverse data sources. An abstraction layer for the applications in these systems are considered as <q>Dataspaces</q> as proposed in <a href="http://dl.acm.org/citation.cfm?id=1107502"><cite>From Databases to Dataspaces</cite></a> article. What constitutes a dataspace is that, it would typically consist of a set of data sources with some relationships between them.</p>

                    <dl>
                        <dt>Participants and Relationships</dt>
                        <dd>The participating data sources in a dataspace can be databases, repositories, web services or software packages. The source data can be structured, semi-structured or unstructured, where some may be set to allow updates, yet others only for reading. Similarly, a dataspace may have different relationship models between its sources, with descriptions at any level of granularity, as well as information on data provenance.</dd>

                        <dt>Catalog and Browse</dt>
                        <dd>Dataspaces may include services to allow different modes of interaction with the data sources. Systems might simply provide services to support access to its catalogue in order to allow users to browse its inventory of data resources. Catalogues may have metadata about the resources such as their original source, used schemas, statistics on the data, creators, various timestamps, licensing, completeness and so forth. This type of provenance data provides a perspective for the users and administrators about the elements in the data sources, as well as assistance in reproduction and quality analysis.</dd>

                        <dt>Search and Query</dt>
                        <dd>One other type of service is meant for discovering data by way of searching and querying. The primary function for these services is to allow users to locate and extract particular information from the data sources. A search service offers relevant results to users based on keyword searches, as well as further exploration of the results. Its purpose is to provide a mechanism to deal with large collections of unfamiliar data using natural languages that users are familiar with. A query service in contrast, is meant to provide a structured way to retrieve or manipulate information in participating data sources. One important differentiating factor between searching and querying is that, a query service can let users formulate far more complex questions about the data and get answers to them.</dd>

                        <dt>Local storage and index</dt>
                        <dd>In order to give the data sources a home and to allow inquiry services, a storage and an accompanying index component is used. Local storages and indexes aid in creating efficient queries, precise access to data, and support for data recovery and availability. Indexes are invaluable in terms of identify information across data sources whether locally or references to objects in the real-world.</dd>

                        <dt>Discovery and extensions</dt>
                        <dd>Another type of dataspace service is the discovery component which is used for relating dataspace participants and consequently allow the system to provide better query results. This component would discover, identify and classify data sources and their content in order to easily locate and refer to items in the dataspace in the future. It is important for this component to monitor and allow an environment to update the schema mappings over time in order to accurately represent the dataspace's assets.</dd>

                        <dt>Pay-as-you-go</dt>
                        <dd>In creating semantic relationships between data sources, the involvement of users is usually focused on taking care of most beneficial efforts first. The system is expected to allow increment improvements based on the knowledge of the underlying data's structure, semantics and relationships between sources. Hence, a <em>pay-as-you-go</em> approach to data integration is employed in dataspaces as complete upfront integration is considered to be difficult and is not a required goal.</dd>
                    </dl>


                    <h3 id="linked-data">Linked Data</h3>
                    <p>One manifestation of the Semantic Web vision is Linked Data. It is the a pragmatic approach to publishing structured data on the Web in order to discover related data from different sources. The original design principles was put forward by <a about="http://csarven.ca/#i" rel="foaf:knows" href="http://www.w3.org/People/Berners-Lee/card#i">Sir Tim Berners-Lee</a> in 2006 as follows:</p>

                    <blockquote about="http://www.w3.org/People/Berners-Lee/card#i" rel="dcterms:creator" href="http://www.w3.org/DesignIssues/LinkedData.html">
                        <ol id="linked-data-design-principles">
                            <li id="linked-data-design-principles_1">Use URIs as names for things</li>
                            <li id="linked-data-design-principles_2">Use HTTP URIs so that people can look up those names.</li>
                            <li id="linked-data-design-principles_3">When someone looks up a URI, provide useful information, using the standards (RDF*, SPARQL)</li>
                            <li id="linked-data-design-principles_4">Include links to other URIs. so that they can discover more things.</li>
                        </ol>
                    </blockquote>

                    <dl id="linked-data-description">
                        <dt id="linked-data-description_http-uris">HTTP URIs</dt>
                        <dd>In a nutshell, the use of URIs allows us to refer to things and concepts, whether they are real or imaginary, in an absolute way. In order to persistently make use of things, <cite>Sir Tim Berners-Lee</cite> proposed that <q href="http://www.w3.org/DesignIssues/Axioms.html#Universality2">any resource of significance should be given a URI</q>. By employing the widely adopted <code>http:</code> URI scheme, the idea for Linked Data sets off in terms of providing a representation for requested resources.</dd>

                        <dt id="linked-data-description_rdf-data-model">RDF data model</dt>
                        <dd>The key ingredient in the information that is returned to the user has to do with the model of the data in the response. Regardless of the syntax that is used, Resource Description Framework (RDF) is essentially an entity-relationship model that provides a way to make statements about the things in our reality. A statement contains three atomic parts, also known as a triple: the <em>subject</em> resource in which the statement is about, followed with a <em>property</em> which is a vocabulary term that describes the type of relationship it has to an <em>object</em> resource. Each of these components are typically represented using HTTP URIs, with the possibility of the object resource being a literal string. In mathematical terms, RDF is a directed, labeled graph, which conceptually depicts a graph of things. What makes this method to make claims about things worthwhile is the act of linking any two URIs together in a particular way. It fundamentally presents an opportunity to discover new resources in an uniform way, whether the resource is in local storage or somewhere else.</dd>

                        <dt id="linked-data-description_rdf-vocabularies">RDF vocabularies</dt>
                        <dd>In RDF triple statements, properties are vocabulary terms that are used to relate a subject to an object. As these resources are accessible via HTTP URIs, when dereferenced they provide a description for the term in use. Some of the well-known vocabularies that are used in Linked Data publishing include: Friend of a Friend (FOAF) to describe people and the things that they do; RDF Data Cube which is used to describe multi-dimensional statistical data; SDMX for the statistical information model; British reference periods, SKOS to describe controlled thesauri, classification schemes and taxonomies; DC Terms for general purpose metadata relations and; VoID to provide metadata on datasets.</dd>

                        <dt id="linked-data-description_sparql">SPARQL</dt>
                        <dd>SPARQL Protocol and RDF Query Language (SPARQL) is a protocol and a query language to retrieve and manipulate RDF data. It can be used to express queries across local and remote data sources, whether the data resides in RDF files or databases. SPARQL queries consist of RDF triple graph patterns written in a fashion similar to Turtle, and allows modifiers for the patterns. In the Linked Data scene, it is common to see publicly accessible SPARQL endpoints where queries are sent and received over HTTP. Federated queries can be written to compute results that span over different SPARQL endpoints on the Web.</dd>
                    </dl>

                    <p>The Linked Data efforts are concerned with publishing and querying all sorts of data that's interconnected in the form of a <em>Giant Global Graph</em>. Some of the motivations behind this is to uncover insights about societies, build smarter systems, making predictions, democratizing data for people, or to make better decisions.</p>


                    <h3 id="linked-statistics">Linked Statistics</h3>
                    <p>With the rise of special-purpose, domain-specific formats such as <a href="http://www.scb.se/Pages/List____314011.aspx">PC-Axis</a> [<a href="#r_1">1</a>] or <a href="http://sdmx.org/">Statistical Data and Metadata eXchange</a> (SDMX) [<a href="#r_2">2</a>], an ISO standard for exchanging and sharing statistical data and metadata among organizations, re-using statistical data has become more possible. However, with the complexity introduced by these formats, the barrier for consuming the data has raised as well. On the other hand, general-purpose formats such as Microsoft's Excel or CSV are very widely deployed and a number of tools and libraries in any kind of programming language one could possibly think of exist to process them. The down-side of these formats is equally obvious: as much of the high-quality annotations and metadata, that is, how to interpret the observations, is not or only partially captured, the data fidelity suffers. Even worse, using these formats, the data and metadata typically gets separated. With linked statistics, one can leverage the existing infrastructure as well as retaining metadata along with the data, yielding high data fidelity, consumable in a standardised, straight-forward way. However, the handling of statistical data as Linked Data requires particular attention in order to maintain its integrity and fidelity.</p>

					<p>Going beyond the operations of slicing, filtering and visualising statistical data typically requires out-of-band information to combine it with other kinds of data. Contextual information is usually not found in the statistical data itself. Using linked statistics, we are able to perform this data integration task in a more straight-forward way by leveraging the contextual information provided by the typed links between the data items of one data set to other datasets in the LOD cloud.</p>

                    <p>The <a href="http://www.w3.org/TR/vocab-data-cube/">RDF Data Cube vocabulary</a> is used to express multi-dimensional statistical data on the Web, and its data model is compatible with the cube model that underlies SDMX. Data cubes are in the nature of a hyper-cube such that multiple dimensions may be used to refer to a particular observation in the cube. Data cubes are characterised by their <em>dimensions</em>, which indicate what the observation is about with a set of properties; its <em>measures</em> to represent the phenomenon that is being observed with a value; and optionally with <em>attributes</em> which help interpret the measure values with a unit. Dimensions typically represent concepts, which are taken from a code list, and are highly valuable as they may used across data cubes by any consumer. Code lists, also known as classifications, are typically identified by using the <em>Simple Knowledge Organization System</em> (SKOS) vocabulary in RDF.</p>

					<p>What linked statistics provide, and in fact enable, are queries across datasets: given the dimensions are linked, one can learn from a certain observation's dimension value, other provided dimension values, enabling the automation of cross-dataset queries, hence cutting down integration costs and delivering results quicker.</p>

					<p>Organisations that are involved in publishing statistical Linked Data and establishing related methodologies and best practices include the <a href="http://data.gov.uk/">UK Government</a> [<a href="#r_3">3</a>], the National Institute of Statistics and Economic Studies (<dfn><abbr title="National Institute of Statistics and Economic Studies">INSEE</abbr></dfn>, France), the U.S. Bureau of Labour Statistics (<dfn><abbr title="Bureau of Labour Statistics">BLS</abbr></dfn>), and the European Environment Agency (<dfn><abbr title="European Environment Agency">EEA</abbr></dfn>). Statistics from many other sources are currently published not by the original statistics producer, but by third parties (universities, web technology companies etc.): U.S. Census 2000, Spanish Census, including historical microdata, EU election results, <a href="http://img.org/">International Monetary Fund</a> (<dfn><abbr title="International Monetary Fund">IMF</abbr></dfn>) commodity prices to name a few as well as the data from Central Statistics Office Ireland, Eurostat, and World Bank, which we will focus on in this paper.</p>


                    <h3 id="linked-data-lifecycles">Lifecycles</h3>
                    <p>Summarize all of the lifecycles: http://www.w3.org/2011/gld/wiki/GLD_Life_cycle</p>
                    <p>Are they generic or government or statistical</p>
                    <ul>
                        <li>Hyland et al.</li>
                        <li>Hausenblas et al. http://linked-data-life-cycles.info/</li>
                        <li>Villazon-Terrazas et al.</li>
                        <li>DataLift vision</li>
                        <li>Linked Open Data Lifecycle (LOD2)</li>
                    </ul>


                    <h2 id="statistical-linked-dataspace">Statistical Linked Dataspace</h2>
                    <p>The idea of <em>Dataspaces</em> as discussed earlier was proposed without being tied to any particular set of technologies or types of data. It is a broad description for what constitutes a dataspace. For the moment, without diving into specific technologies, tooling, or data, possible parallels can be drawn between <em>Dataspaces</em> and dataspaces which follows the Linked Data design principles. Table [<a href="#table_dataspaces-to-linked-dataspaces">1</a>] presents a generalized view for the components and services in Linked Dataspaces.</p>

                    <table id="table_dataspaces-to-linked-dataspaces">
                        <caption><strong>Table 1.</strong> Dataspaces to Linked Dataspaces</caption>
                        <thead>
                            <tr><th>Dataspaces</th><th>Linked Dataspaces</th></tr>
                        </thead>
                        <tbody>
                            <tr><td>Catalog</td><td>RDF dataset descriptions</td></tr>
                            <tr><td>Browse</td><td>HTML, RDF publishing framework</td></tr>
                            <tr><td>Local store and index</td><td>RDF files, stores, HTTP URIs</td></tr>
                            <tr><td>Search and Query</td><td>Free-text search and SPARQL</td></tr>
                            <tr><td>Discovery and relationships</td><td>Link discovery framework</td></tr>
                            <tr><td>Data management extensions</td><td>Semi-automatic data deployment</td></tr>
                            <tr><td>Metadata and provenance</td><td>RDF metadata vocabularies</td></tr>
                        </tbody>
                    </table>

                    <p>As Dataspaces are not attached to any specific list of data deployment services or practices, the Linked Dataspace can be seen as a narrower or a particular realization of a Dataspace. Having a dataspace that adheres to Linked Data principles is that, one of the forefront goals is to offer the data sources and some of its services in a way that the underlying data can be globally accessed and linked to by external data sources and applications.</p>

                    <p>The proposal here is that, with the assumption of publishing statistical data in a way that its components can be identified, discovered, and disseminated for numerous uses, a Linked Dataspace can be an ideal candidate. A statistical Linked Dataspace needs to identify resources and provide access to their descriptions, where such resources are in the nature of code lists, data cube observations, datasets and structures. Some of the specific challenges include: extraction of original data sources, transformations to RDF, and loading (ETL) to data storage services; creating global identifiers for the resources in its data catalogs; using vocabularies which are designed for modeling statistical data; offering services to the outside world such that its participating data sources can be browsed through; discovery via text searches and structured queries; and building interlinks with other data sources.</p>


                    <figure id="figure_linked-dataspaces">
                        <figcaption><strong>Figure 9</strong>: Linked Dataspaces architecture</figcaption>
                        <object type="image/svg+xml" data="linked-dataspaces.svg" width="420" height="280"></object>
                    </figure>

                    <h2 id="requirements">Requirements</h2>
<!--
                    <p>Analysis of the original datasets (CSO Ireland, Eurostat, World Bank): What's available, why and what we are interested in (, what we need to prepare to access them?)</p>
                    <h3>Quantity: Huge amount of observations.. characteristics of the datasets (codelists.. in order to interpret observations).. how frequently are they updated (lower-upper bound)? Quality issues.. Provenance.. Not technically. Understanding / Analysis of the datasets.. How are they available (dumps, APIs)</h3>
-->
                    <p>A set of requirements to create a statistical Linked Dataspace are derived from the case studies of deploying CSO Ireland, Eurostat, and World Bank Linked Data. The requirements will be based on analysis of the original data sources, such as their characteristics in terms of quality, quantity, and update frequency.</p>

                    <p>A high-level inspection of published statistical data reveals itself as a collection of significant amounts of data, that's compiled over time by different parties. Naturally the quality of the data sources vary from one publisher to the next, as they try to cater to different publishing criteria. For instance, statistical data is often available in different data formats (e.g., CSV, XML, JSON, SDMX-ML, PC-Axis), with varying vocabularies and thesaurus that's specific to publisher's view about the data. In terms of consumption, one particular example is as follows: the usefulness of raw data that is available in CSV format may depend on the availability and quality of the metadata for the terms that are used in the file. The shape in which the data is available, sets the tone for the data consumer. The data consumers need to make decisions based on the amount of work involved to obtain and work with such data in their own space.</p>

<div class="todo">
The shape in which the data is available, sets the tone for the data consumer.> ... characteristics of the data..
here is the original data.. and here is what we are trying to do with the data..
.. fresh data, such and such interface on top of it because that makes sense for the users of the data, we don't want to spend so much time doing x,y,z.
1. what's in the data
2. what do you want to do with it?
</div>


                    <p>Therefore, it is reasonable to conclude that a number of decisions need to be made in order to prepare the dataspace for retrieval and reuse. A questionnaire along the following the lines can help determine core requirements by analysing each case study:</p>
                    <dl id="requirements-questionnaire">
                        <dt id="requirements-questionnaire_overview">Overview</dt>
                        <dd>Who is the data publisher?</dd>
                        <dd>Which parties was involved in compiling the original data?</dd>
                        <dd>What data is being published?</dd>
                        <dd>What's the importance of that data?</dd>
                        <dd>Who are the people to contact?</dd>
                        <dd>Is the accuracy of the data indicated?</dd>
                        <dd>Are there different versions or variations of the data?</dd>

                        <dt id="requirements-questionnaire_retrieval">Retrieval</dt>
                        <dd>Which formats is the data available in?</dd>
                        <dd>How big is the data?</dd>
                        <dd>How frequently is the data updated, if at all?</dd>
                        <dd>How many retrieval requests need to be made?</dd>
                        <dd>Which access points is the data distributed through?</dd>
                        <dd>Are there special access privileges required to retrieve the data?</dd>

                        <dt id="requirements-questionnaire_processing">Processing</dt>
                        <dd>Is the necessary tooling in place in order to work with the available formats?</dd>
                        <dd>Are the data files syntactically well-formed?</dd>

                        <dt id="requirements-questionnaire_publication">Publication</dt>
                        <dd>What's the data license and terms of use?</dd>
                        <dd>Is there sufficient metadata?</dd>
                        <dd>Which languages is the data and metadata available in?</dd>
                        <dd>What granularity is the data in?</dd>
                    </dl>


                    <h3 id="data-source">Data sources</h3>
                    <p>The following initial observations are made about the data sources:</p>

                    <h4 id="data-source_cso-ireland">CSO Ireland</h4>
                    <p>The <a href="http://cso.ie/">Central Statistics Office</a> (<dfn><abbr title="Central Statistics Office">CSO</abbr></dfn>) [<a href="#r_4">4</a>] is the official Irish agency responsible for collecting and disseminating statistics about Ireland. The main source of the statistical data for the CSO is the National Census that is scheduled to be held every five years. The data compiled by the CSO serve as a key input for decision-making in the Irish government and it informs its policies and programmes both at national and local levels.</p>

                    <p>The CSO publishes population statistics in several ways, none of which is particularly suited for direct reuse. The data is primarily available through the CSO's website, formatted for the purpose of display. The CSO offers access to raw demographic data in PC-Axis format for expressing multidimensional statistical data. CSO exposes raw data in an interactive data viewer provided by the <a href="http://www.beyond2020.com/">Beyond 20/20 software</a> [<a href="#r_5">5</a>] that allows to browse, sort and plot the data. It offers a way to export the data in XLS and CSV.</p>

                    <h4 id="data-source_eurostat">Eurostat</h4>
                    <p><a href="http://ec.europa.eu/eurostat">Eurostat</a> [<a href="#r_6">6</a>] is the statistical office of European Union with the aim to provide European Union statistical information in a way that can be comparable at European level. Statistical data collection is done by statistical authorities of each Member States. They verify and analyse the data before sending it to Eurostat. Eurostat's role is to consolidate the statistical data they receive from each Member States and ensure that they are comparable. Eurostat actually only provides harmonized statistical data using common statistical language.</p>

                    <p>Eurostat offers access to datasets using the <a href="http://epp.eurostat.ec.europa.eu/NavTree_prod/everybody/BulkDownloadListing">bulk download facility</a> [<a href="#r_7">7</a>]. The datasets are published by Eurostat in three different formats: TSV, DFT and SDMX. This makes it possible for users to import the data into the tool of their choice. A complete list of datasets which are published by Eurostat is made available through table of contents. Although there is no filtration on the different types of statistics provided by Eurostat, the datasets essentially cover statistical information along the following themes: general and regional statistics, economy and finance, population and social conditions, industry, trade and services, agriculture and fisheries, external trade, transport, environment and energy, and science and technology.</p>

                    <h4 id="data-source_world-bank">World Bank</h4>
                    <p>The <a href="http://worldbank.org/">World Bank</a> [<a href="#r_9">9</a>] is an international development organization that provides access to a comprehensive set of data about countries around the globe. The publicly available statistical data is collected from officially-recognized international sources, and consists of a wide array of observations on development indicators, financial statements, climate change, projects and operations.</p>

                    <p>The World Bank provides a free and open access to numerous datasets in their <a href="http://data.worldbank.org/data-catalog">data catalog</a> [<a href="#r_10">10</a>]. These datasets are available in one or more formats: XML, JSON, CSV, XLS; with additional geospatial data in SHP and KML, and supporting documentations in PDF. The World Bank APIs offers some of the datasets primarily in XML and JSON representations, whereas the rest of the formats are available as data dumps. In our use-case, the decision on which datasets to work with was based on several factors such as the importance of the dataset, its completeness, and the ease of converting it into an RDF representation. Hence, the following datasets from the World Bank's API was selected with the preference of working with XML:</p>

                    <ul>
                        <li><a href="http://data.worldbank.org/developers/climate-data-api">World Bank Climate Change</a> (<dfn><abbr title="World Bank Climate Change">WBCC</abbr></dfn>) [<a href="#r_11">11</a>] contains data from historical observations and future projections derived from global circulation models.</li>
                        <li><a href="http://data.worldbank.org/data-catalog/world-development-indicators">World Development Indicators</a> (<dfn><abbr title="World Development Indicators">WDI</abbr></dfn>) [<a href="#r_12">12</a>] contain various global development data on world view, people, the environment, the economy, states and markets, and global links. It includes national, regional and global estimates.</li>
                        <li><a href="https://finances.worldbank.org/">World Bank Finances</a> (<dfn><abbr title="World Bank Finances">WBF</abbr></dfn>) [<a href="#r_13">13</a>] cover Bank's investments, assets it manages on behalf of global funds, and the Bank's own financial statements.</li>
                        <li><a href="http://data.worldbank.org/data-catalog/projects-portfolio">World Bank Projects and Operations</a> (<dfn><abbr title="World Bank Projects and Operations">WBPO</abbr></dfn>) [<a href="#r_14">14</a>] provides information about the lending projects from 1947 to present along with links to publicly disclosed online documents.</li>
                    </ul>



                    <h3 id="core-requirements">Core requirements</h3>
                    <p>The following core requirements are distilled from the deployment of CSO Ireland, Eurostat, and World Bank Linked Dataspaces:4</p>

                    <dl>
                        <dt id="core-requirements_storage">Storage space and RDF store</dt>
                        <dd><em class="rfc2119">Must</em> have storage space where the retrieved raw data will be stored and processed in the file system, as well as an RDF storage and server to house the transformed RDF data.</dd>
                        <dt id="core-requirements_transformation-tools">Transformation tools</dt>
                        <dd><em class="rfc2119">Must</em> have a set of tools to preprocess and convert non-RDF structured data to RDF. This primarily includes tools that would allow inspections, modifications, and conversion of the data into other formats.</dd>
                        <dt id="core-requirements_enrichment-tools">Enrichment tools</dt>
                        <dd><em class="rfc2119">Must</em> have a set of tools that can help to improve on the existing data. In order to fulfil the <a href="#linked-data-design-principle_4">4<sup>th</sup> Linked Data design principle</a> some of the important resources in the data needs to be interlinked with resources elsewhere. Therefore, the dataspace should either have tools to semi-automate the discovery of potential resources, or provide ways to manually find such resources. Another way to enrich existing data would be by way of aggregating related or useful data that's not in the original data.</dd>
                        <dt id="core-requirements_vocabularies">Vocabularies</dt>
                        <dd><em class="rfc2119">Must</em> have an RDF vocabulary to describe the statistical data model. <em class="rfc2119">Should</em> have a set of vocabularies that can describe the relationships in the data, code lists, and metadata for the datasets.</dd>
                        <dt id="core-requirements_data-synchronisation">Data Synchronisation</dt>
                        <dd><em class="rfc2119">Should</em> keep up to date with the changes that are made to the original data. This is important as far as making sure that the data is accurate and relevant.</dd>

                        <dt id="core-requirements_publication">Publication</dt>
                        <dd><em class="rfc2119">Must</em> publish the data that's consumable in any one of the standardized RDF formats, and allow SPARQL queries to be run over the data given the <a href="#linked-data-design-principle_3">3<sup>rd</sup> Linked Data design principle</a>. <em class="rfc2119">May</em> provide RDF data dumps in order to allow data consumers to easily retrieve the full dataset. <em class="rfc2119">Should</em> provide an HTML interface to browse through the data or its catalog for human consumption.</dd>
                    </dl>


                    <h2 id="components">Components</h2>
                    <p>Given the requirements to publish a statistical Linked Dataspace, Table 9 outlines a list of required components to realize such space with example implementations.</p>

                    <table id="linked-dataspace-components-implementations">
                        <caption><strong>Table 9.</strong> Linked Dataspace components, and implementations</caption>
                        <thead>
                            <tr><th>Components</th><th>Implementations</th></tr>
                        </thead>
                        <tbody>
                            <tr><td>Environment</td><td>OS: Linux, Mac, Windows; Server: Apache, IIS</td></tr>
                            <tr><td>Data retrieval</td><td>CURL, Java, LDSpider, wget</td></tr>
                            <tr><td>Data preprocessing</td><td>Manual with custom scripts, Google Refine</td></tr>
                            <tr><td>Data modeling</td><td>Google Refine, Manual, Neologism</td></tr>
                            <tr><td>Data conversion</td><td>XSLT, CSV2RDF, D2R</td></tr>
                            <tr><td>RDF store and SPARQL servers</td><td>TDB, Fuseki, Virtuoso, Sesame, 4store</td></tr>
                            <tr><td>Loading RDF data to RDF store</td><td>Manual with custom scripts</td></tr>
                            <tr><td>RDF store optimization</td><td>TDBstats</td></tr>
                            <tr><td>Interlinking</td><td>Manual, LIMES, Silk</td></tr>
                            <tr><td>Enrichment</td><td>Manual, Stanbol</td></tr>
                            <tr><td>User interface</td><td>Callimachus, Linked Data API, OpenLink Data Explorer, Pubby</td></tr>
                            <tr><td>Database metadata</td><td>VoID, LODStats</td></tr>
                            <tr><td>Data dumps</td><td>File system</td></tr>
                            <tr><td>Applications</td><td>LodLive, SGVizler, Tabulator</td></tr>
                        </tbody>
                    </table>

                    <p>For some of the components, there are several implementations that are publicly available for use. A range of improvements can be done for each implementation with varying complexity. In the case studies, two areas in particular were identified for possible areas for improvements given the state of the art of publicly available technologies: loading of RDF data to RDF store, and a human user interface.</p>
                    <p>It turned out that loading RDF data was a recurring task that had to be performed by the administrator. Hence, the <a href="#graphpusher">GraphPusher</a> tool was built to improve on the manual approach by automating bulk data. GraphPusher was primarily used to help to deploy the Irish Government Linked Dataspace at <a href="http://data-gov.ie/">DataGovIE</a> and this is a report on preliminary findings on using this tool. As discussed in data sources for CSO Ireland, the data primarily consists of statistical data about the Irish population.</p>

                    <p>On a similar note, <a href="#linked-data-pages">Linked Data Pages</a> was written to provide better control over front-end templating system to present a Web interface for RDF data. The Linked Data Pages framework was used to publish CSO Ireland and <a href="http://worldbank.270a.info/">World Bank</a> Linked Dataspaces.</p>

                    <p>Both of these contributions are discussed in further detail in the following sections.</p>

                    <h3 id="graphpusher" class="todo" title="Extend in-context of Statistical Linked Data.">GraphPusher</h3>
                    <p>The manual creation of dataspaces [<a href="#r_1">1</a>] conforms to Linked Data principles is time consuming, and prone to human errors since there are a number of steps involved to gather the data, and place them in a store. Typically a list is compiled consisting of datasets with graph names for each dataset, and local copies of the data that is to be imported in to a store. This information may be tracked in a structured format, or a simple text file. The Vocabulary of Interlinked Datasets (VoID) [<a href="#r_2">2</a>] is used because it is an accepted standard to describe RDF datasets, as well as the SPARQL 1.1 Service Description (SD) [<a href="#r_3">3</a>] for discovering information about a SPARQL service.</p>

                    <p>One of the goals of publishing a VoID file alongside datasets that are published as Linked Data is to allow consumers to discover and retrieve the data. Based on this, the GraphPusher tool [<a href="#r_4">4</a>] is designed to aid users to retrieve and import data in RDF format into a graph store by making use of the metadata in a VoID file. VoID describes access methods for the actual RDF triples in a number of different ways. GraphPusher focuses on two of these methods; dereferencing HTTP URIs and compressed RDF data dumps.</p>

                    <p>Given RDF data that is to be imported into a graph store, additional information is required in order to carry out the update process; how to access the store, dataset to store in, and graph name to use. It naturally follows that this unique information needs to be handed over to the store per dataset for the update. When dataspaces are updated frequently, it brings forward a reason to perform with a script, minimizing human involvement and errors. Therefore, the underlying purpose for GraphPusher is to help with data deployment as efficiently as possible.</p>

                    <p>Where users are publishers and consumers at the same time, the creation of a VoID description can be seen as a declarative programming approach to putting RDF data into data stores. GraphPusher&#8217;s potential use is to help applications to pull in Linked Data.</p>

                    <p>GraphPusher was originally created to fulfil the data deployment need of DataGovIE&#8217;s [<a href="#r_5">5</a>] production and staging datasets. DataGovIE reuses its published VoID description to feed its own dataspace with incremental changes to data.</p>

                    <p>GraphPusher approaches this task with the following sequence. Figure [<a href="#diagram_graphpusher-sequence">1</a>] gives an illustration.</p>

                    <ol>
                        <li>Compiling a list of datasets to be retrieved to local space and graph name to use per dataset or file. This is accomplished by retrieving a VoID file and extracting the <code>void:dataDump</code> property values. It also looks for SD&#8217;s <code>sd:name</code> property in VoID, where they are collected to name the graphs in the RDF store. In the case that <code>sd:name</code> is not present, the graph name method is determined by user&#8217;s configuration in GraphPusher.</li>

                        <li>Datasets are downloaded to local disk. For compressed dataset archives, they are decompressed.</li>

                        <li>Finally, the data is imported into the graph store by either executing an update operation on the RDF store directly, or through the SPARQL service via SPARQL Update.</li>
                    </ol>

                    <figure id="figure_graphpusher-sequence">
                        <figcaption><strong>Figure 1</strong>: GraphPusher sequence</figcaption>
                        <object type="image/svg+xml" data="graphpusher-sequence.svg" width="300" height="220"></object>
                    </figure>

                    <h4 id="graphpusher_implementation">Implementation</h4>
                    <p>A reoccurring dataspace operation in Linked Data environments is the retrieval of remote data and placing them in an RDF storage. In the case of rebuilding an RDF store, the approach is to use a VoID description for the datasets in order to batch process some of the recurring steps. This is typically accomplished by publishing a VoID description of the datasets with triples using the <code>void:dataDump</code> property, and optionally with <code>sd:graph</code> and <code>sd:name</code> properties.</p>

                    <h4 id="graphpusher_related-work">Related work</h4>
                    <p>To the best of publicly available practices, there exists no technology that builds a dataspace automatically. The work is based on VoID as SADDLE [<a href="#r_11">11</a>] and DARQ [<a href="#r_12">12</a>] do not provide the required metadata coverage for dataset access, and VoID has 30% coverage in the Linking Open Data (LOD) [<a href="#r_13">13</a>] and is actively maintained.</p>

                    <h4 id="graphpusher_about-the-code">About the code</h4>
                    <p>GraphPusher takes a VoID URL as input from the command-line, retrieves the VoID file, looks for the <code>void:dataDump</code> property values, HTTP GETs them, and finally imports them into an RDF store using one of the graph name methods. The graph name method is defined as part of GraphPusher&#8217;s configuration.</p>

                    <p>In order to specify the location of the RDF store for the data that is to be imported, GraphPusher can take either TDB Triple Store&#8217;s [<a href="#r_6">6</a>] assembler file which contains configuration for graphs and datasets, or a SPARQL service URI. If the TDB assembler filename is provided in the command-line argument with <code>--assembler</code>, GraphPusher checks this file to determine the name of the dataset and its location. It will then import the data dumps using the TDB loader. Alternatively, with the <code>--dataset</code> option, GraphPusher uses Apache Jena&#8217;s SPARQL over HTTP command-line script [<a href="#r_7">7</a>] to HTTP POST by using the SPARQL 1.1 Graph Store HTTP Protocol [<a href="#r_8">8</a>].</p>

                    <p>GraphPusher is written in Ruby, and relies on the rapper [<a href="#r_9">9</a>] RDF parser utility, and optionally the TDB Triple Store. GraphPusher is configurable, e.g., location to store the data dumps, method to determine named graphs. It decompresses archived data dumps to a local directory, converts files with unrecognized RDF format extensions to Turtle format before importing. It is tested and functional in Debian/Ubuntu systems, available under the Apache License 2.0 [<a href="#r_10">10</a>].</p>

                    <h4 id="graphpusher_determining-graph-name">Determining graph name</h4>
                    <p>Where possible, GraphPusher makes a decision for dataset&#8217;s named graphs by staying consistent with publishers&#8217; intentions. In certain cases, it tries to indicate the origin of the data by reusing the dataset location in the graph name.</p>

                    <p>Figure [<a href="figure_named-graph-flowchart">2</a>] illustrates the flow for this process.</p>

                    <p>If <code>sd:name</code> is present in VoID, its value has highest priority because it explicitly states the named graph that is used in the dataset, and consequently how it can be referenced in a <code>FROM/FROM NAMED</code> clause.</p>

                    <p>If <code>sd:name</code> is not present, GraphPusher looks for one of the <code>graphNameMethod</code> configuration property values: <code>dataset</code>, <code>dataDump</code>, <code>filename</code>. By default <code>dataset</code> value tells the GraphPusher to simply use the URI of the <code>void:Dataset</code>. If <code>dataDump</code> is set, the named graph IRI becomes the IRI of the data dump. Alternatively, if the <code>filename</code> method is used, the file name is appended to the <code>graphNameBase</code> URI value.</p>

                    <figure id="figure_named-graph-flowchart">
                        <figcaption><strong>Figure 2</strong>: Determining named graph flowchart</figcaption>
                        <object type="image/svg+xml" data="graphpusher-named-graph-flowchart.svg" width="360" height="340"></object>
                    </figure>

                    <h4 id="graphpusher_preliminary-results">Preliminary Results</h4>
                    <p>As there exists no automated approach to have local copies of the data dumps, determining a graph name per data dump, and importing the data into a store, the manual approach is used as baseline to compare against. The following criteria is used for comparison:</p>
                    <dl>
                        <dt>Configurability</dt>
                        <dd>Indicates the flexibility and limitations of the approaches.</dd>
                        <dt>Requirements</dt>
                        <dd>Knowledge that is required to perform the tasks such as configuring and running common Linux shell tools, as well as VoID.</dd>
                        <dt>Efficiency</dt>
                        <dd>This criteria is intended to provide a measurement for the amount of time it takes to perform the full data loading process. It is context of performing the task more than once.</dd>
                    </dl>

                    <p>Table [<a href="#table_comparison-manual-graphpusher-approaches">1</a>] gives an overview for the comparison of manual and GraphPusher approaches.</p>

                    <table id="table_comparison-manual-graphpusher-approaches">
                        <caption><strong>Table 1</strong>: Comparison of manual and GraphPusher approaches</caption>
                        <thead>
                            <tr><td></td><th>Manual</th><th>GraphPusher</th></tr>
                        </thead>
                        <tbody>
                            <tr><th>Configurable</th><td>Full</td><td>Predefined</td></tr>
                            <tr><th>Requirements</th><td>None</td><td>VoID</td></tr>
                            <tr><th>Efficiency</th><td colspan="2"><a href="#graphpusher_efficiency">See discussion below</a></td></tr>
                        </tbody>
                    </table>

                    <p><strong>Configurability</strong>: The manual approach allows the user to configure the loading process as they see fit, whereas the GraphPusher approach comes with a predetermined list of configuration options to import the data. In that sense, GraphPusher is naturally limited to its feature set.</p>

                    <p><strong>Requirements</strong>: The bare-bone manual approach would typically make use of <em>wget</em> or <em>curl</em> to download the dataset files, several archive tools to decompress compressed datasets, and an RDF parser to convert a given dataset file in an RDF format to N-Triples format, where it is suitable for pattern matching with the <em>grep</em> tool.</p>

                    <p id="graphpusher_efficiency"><strong>Efficiency</strong>: The steps mentioned above would require the user to posses knowledge of command-line operations and running SPARQL queries, and depending on their expertise and available tools, given test runs, it takes 20 minutes or more if executed by hand. The automated approach in GraphPusher would typically take a minute or two to configure and run from command-line. The efficiency of each approach is noteworthy, especially when this task is repeated for different datasets and metadata.</p>

                    <p>It should be noted here that, the expertise and time needed to have the required tools set up in the system is excluded as they vary from one favourable approach to another in a given operating environment. VoID is GraphPusher&#8217;s primary requirement, hence the assumption is that a VoID file exists, or can be generated.</p>

                    <h4 id="graphpusher_conclusions">Conclusions</h4>
                    <p>GraphPusher illustrates a direct application of reusing a VoID description to pull in datasets to a dataspace. At its core, it takes advantage of RDF&#8217;s <em>follow your nose</em> discovery to support data digestion pipelines. This is in line with the Linked Data methodology as it mostly relies on dereferenceable HTTP URIs.</p>

                    <p>Given the following limitations of GraphPusher, there is future work in plans:</p>
                    <ul>
                        <li>Retrieval of a VoID description in an RDF store via a SPARQL Endpoint.</li>
                        <li>Ability to skip data dump retrievals by checking for datasets&#8217; timestamps in HTTP headers, and using local copies where appropriate.</li>
                        <li>Techniques to optimize storage footprint for duplicate copies of retrieved datasets.</li>
                        <li>Merging all dump files per graph into a single N-Triples file before importing to the RDF store for more efficient loading. Consequently, this approach needs to account for blank nodes which occur in different files with the same identifier.</li>
                        <li>Support other popular RDF stores e.g., Virtuoso RDF Quad Store [<a href="#r_14">14</a>] and 4store [<a href="#r_15">15</a>], to directly communicate with the store &#8211; currently TDB Triple Store is supported.</li>
                    </ul>

                    <h3 id="linked-data-pages" class="todo" title="Add a screenshot of an HTML view">Linked Data Pages</h3>

                    <p>Figure 8 illustrates a general process from requested URI (input) and response in HTML or RDF (output).</p>
                    <figure id="figure_linked-data-request-response">
                        <figcaption><strong>Figure 8</strong>: Linked Data Request and Response interface</figcaption>
                        <object type="image/svg+xml" data="linked-data-request-response.svg" width="240" height="190"></object>
                    </figure>

                    <figure id="figure_linked-data-pages_architecture">
                        <figcaption><strong>Figure 8</strong>: Linked Data Pages architecture</figcaption>
                        <object type="image/svg+xml" data="linked-data-pages_architecture.svg" width="365" height="290"></object>
                    </figure>


                    <p>Linked Data Pages (LDP) is a Web publishing tool to generate RDF and HTML documents. It invokes unique SPARQL queries based on requested URIs and outputs the results to corresponding HTML templates. Links to alternate RDF formats as well as JSON are handled by content-negotiation.</p>

                    <h4>Motivation</h4>
                    <p>Initial reasons to develop LDP in 2010 was to move beyond the de facto generic entity rendering for RDF resources That is, the requested URI corresponded with the result of <code>DESCRIBE</code> SPARQL query, where the property and object resources were primarily displayed in a key-value fashion. </p>

                    <p>While allowing default page rendering as base, if the developer had knowledge about the resources or the data model, it was reasonable to provide a way for them to uniquely display the resources to be more human friendly.</p>

                    <h4>Overview</h4>
                    <p>It is built on top of <a about="http://iandavis.com/id/me" rev="foaf:maker" href="http://code.google.com/p/paget/">Paget</a>, <a href="http://code.google.com/p/moriarty/">Moriarty</a>, and <a href="https://github.com/semsol/arc2">ARC2</a> libraries. Paget is a versatile Web application written in PHP by <a about="http://csarven.ca/#i" href="http://iandavis.com" rel="foaf:knows" resource="http://iandavis.com/id/me">Ian Davis</a> [http://blog.iandavis.com/2008/10/01/publishing-linked-data-with-php/]. Its HTML rendering of RDF is primitive in a sense that information is display in a tabular way. LDP extends on the framework to output more human readable Web pages. Additionally, LDP relies on its URI dispatch mechanism and to build a local index of the RDF data that's gathered from SPARQL query results.</p>

                    <p>Describe the necessary steps for the entity. Compare to other approaches (Sig.ma). See ODE for different types of entities. Perhaps even from other fields. Heterogeneous data sources in a unified view.</p>


                    <h4 id="linked-data-pages_installation">Installation</h4>
                    <p>There is an installation web page to get LDP going. The same set of configuration settings in the script can be entered directly in the configuration file. The configuration is fairly basic and primarily consists of the following:</p>

                    <dl>
                        <dt>Directories</dt>
                        <dd>Directory paths where each of the libraries (LDP, Paget, Moriarty and ARC2) reside in the file system.</dd>
                        <dt>SPARQL service URI</dt>
                        <dd>This is typically the URI that the SPARQL service accepts queries under.</dd>
                    </dl>

                    <h4 id="linked-data-pages_site-configuration">Site configuration</h4>
                    <p>The Web service can be configured with the following parameters:</p>

                    <dl>
                        <dt>Site settings</dt>
                        <dd>Site name and domain name, root URI for access, theme to use, and site logo.</dd>
                        <dt>URI host mapping</dt>
                        <dd>If the host name that the service is running under is different from the host name that is used in the URIs in the RDF data, this setting allows mapping between the the two. For example, if there is a request is made to <code>http://example.org/foo</code>, the setting can be set to look for <code>http://example.net/foo</code> in the RDF store instead.</dd>
                    </dl>

                    <p>LDP is bundled with a default CSS theme, but allows different stylesheets to be used by simply providing its location.</p>

                    <p>The reason to develop the URI host mapping feature was based on the fact that in the early stages of modeling 2006 Census data, the URI patterns was designed based on the knowledge of having the data work under <code>govdata.ie</code>. During the staging phase of publishing the RDF data under <code>data-gov.ie</code>, the URI patterns were not changed until further notice.</p>


                    <h4 id="linked-data-pages_resource-configuration">Resource configuration</h4>
                    <p>LDP allows prefixes to be assigned for the URIs in the configuration file. It is put to use when working with query response values inside templates.</p>

                    <p>LDP provides a way to organize what the URIs should describe and how they should be presented for the user in HTML and RDF. This is accomplished by putting three components together. The first is the URI pattern in question that is being offered, the second is the SPARQL query that it triggers when requested, and third is the template that is to be used to render the HTML page. Definitions of each of the components are defined as such:</p>

                    <dl>
                        <dt>URI pattern</dt>
                        <dd>Patterns are triggered in a way that when a URI is recognized, the most precise URI is selected for the response. This is in contrast to more relaxed patterns which are used to display. For instance, for code lists, each concept URI can be rendered the same way, such that it makes sense catch all concepts at their root path (code list) as opposed their full path. e.g., When <code>http://worldbank.270a.info/classification/country/CA</code> is requested, there would be a URI pattern looking out for <code>http://worldbank.270a.info/classification/country/</code> to respond with an output for all countries the same way. Of course, if one wishes to render <code>http://worldbank.270a.info/classification/country/CA</code> differently than the other countries, it can do so by using that URI pattern in the configuration.</dd>
                        <dt>SPARQL Queries</dt>
                        <dd>Once the URI pattern is determined, a SPARQL query needs to be triggered to retrieve a description of the resource. A full blown SPARQL query pattern can be entered here for each resource. If there is no URI pattern matched, it will default to <code>DESCRIBE &lt;URI&gt;</code>.</dd>
                        <dt>HTML templates</dt>
                        <dd>When an URI pattern is matched, along with the SPARQL query to trigger, an HTML template is assigned at the same time. The template has access to the response gathered from the SPARQL query. An index of values can be either be accessed directly, or a set of helper methods can be used directly from the template. For example,the <code>getTriples</code> function is used to find triples in an index (a multi-dimensional array). When no parameters are provided, it will simply return the full response. It can be used to look for a particular triple in the response. <code>$triples = $this->getTriples('http://csarven.ca/#i');</code> can be used directly in the template to get all of the triples with subject <code>http://csarven.ca/#i</code>. Similarly, <code>$this->getTriples('http://csarven.ca/#i', 'http://xmlns.com/foaf/0.1/knows');</code> can be used to get triples which match the pattern with given subject and property URIs. An alias to this is the <code>getValue</code> function where qnames can be used in the property position: <code>$this->getValue('http://csarven.ca/#i', 'foaf:knows');</code>. For more complex templating, for more data processing with PHP, the <code>SITE_Template</code> class offers more helper functions and can be extended.</dd>

                        <dd>If there is no URI pattern matched, the default template will be used to output the results. Given that the SPARQL query will use <code>DESCRIBE</code>, the default rendering of this template is displayed in a key-value pair fashion.</dd>
                    </dl>

                    <p>This resource configuration approach provides a flexible way for site owners to prepare responses for URIs as well as unique user interfaces.</p>


                    <h4 id="linked-data-pages_conclusions">Conclusions</h4>
<p class="todo">Differentiate Conceptual presentation vs. implementation ??????</p>

                    <p>From the implementation side, Linked Data Pages provides results based on the URI pattern that is triggered. In other words, what information a resource contains and how it is represented is determined at the level of the URI itself. An alternative approach to outputting an HTML document would be based on the characteristics of the resource at hand. That is, the triple graph pattern from a SPARQL query result would allow the developer to shift through the results and assign a template in context of the triples at hand.</p>

                    <p>LDP is capable of taking care of the bulk of the work that's required to publish Linked Data. The <a href="http://csarven.ca/how-to-create-a-linked-data-site">how to create a Linked Data site</a> explains the full process in further detail, from setting up a SPARQL service, importing data, to publishing pages using LDP.</p>





                    <h2 id="statistical-linked-dataspace_case-studies">Realization of a statistical Linked Dataspace</h2>
<div class="todo">
                    <p>Describe from the deployment side.. Why TDB, LIMES..</p>
                    <p>Explain how I've built these spaces</p>
</div>


                    <h3 id="architecture">Deployment Architecture</h3>
                    <p>Figures [<a href="#figure_architecture_cso-ireland">1</a>], [<a href="#figure_architecture_eurostat">2</a>], [<a href="#figure_architecture_world-bank">3</a>] illustrate an overview on the overall architecture of the deployment of our three case studies. They will be discuss in further detail in the following sections.</p>

                    <figure id="figure_architecture_cso-ireland">
                        <figcaption><strong>Figure 1</strong>. CSO Ireland deployment architecture</figcaption>
                        <object type="image/svg+xml" data="cso-architecture.svg" width="460" height="111"></object>
                    </figure>

                    <figure id="figure_architecture_eurostat">
                        <figcaption><strong>Figure 2</strong>. Eurostat deployment architecture</figcaption>
                        <object type="image/svg+xml" data="eurostat-architecture.svg" width="460" height="253"></object>
                    </figure>

                    <figure id="figure_architecture_world-bank">
                        <figcaption><strong>Figure 3</strong>. World Bank deployment architecture</figcaption>
                        <object type="image/svg+xml" data="wb-architecture.svg" width="460" height="176"></object>
                    </figure>

                    <h3 id="data-retrieval">Data Retrieval</h3>
                    <h4 id="data-retrieval_cso-ireland">CSO Ireland</h4>
                    <p>Data from the 2006 Irish Census was retrieved manually, downloading each slice of small area population statistics individually via the export to Excel files from the <a href="http://beyond2020.cso.ie/census/ReportFolders/ReportFolders.aspx">CSO's instance of the Beyond 20/20</a> [<a href="#r_15">15</a>]. 14 datasets in CSV format in total of 8MB was manually retrieved using the interactive application by clicking on the access URLs.</p>

                    <h4 id="data-retrieval_eurostat">Eurostat</h4>
                    <p>There are approximately 6100 datasets published by Eurostat. Eurostat updates information about datasets as well as table of contents twice a day. The datasets holds statistics on daily, monthly, quarterly and annual basis. Therefore, certain datasets are updated daily and many datasets are updated on monthly basis. To keep Eurostat RDF datasets up to date, we've scheduled a cronjob, which runs a set of scripts on weekly basis. In order to avoid unnecessary RDF transformations of each dataset, we've only updated the changed datasets within the past week.</p>

                    <p>Along with the datasets, Eurostat also publishes Data Structure Definitions (<dfn><abbr title="Data Structure Definition">DSD</abbr></dfn>) about each dataset, as well as a set of dictionaries (code lists) which are shared among all datasets. Hence we take into consideration every type of information provided by Eurostat. The DSDs are published in XML format while the code lists are published in TSV format. Dataset is available in three different formats: TSV, DFT and SDMX. Given that the datasets, metadata and code lists each provide different type of information and is represented differently, we wrote Java programs to process XML and TSV formats of the metadata, dataset and code lists separately.</p>

                    <p>We have written different shell scripts which wrap each Java program and one main script which handles the whole process of data downloading and transformation by invoking other scripts.</p>

                    <p>Over 13000 HTTP GET requests was made to Eurostat to download raw datasets, DSDs and code lists, with a total of disk space of ~58GB.</p>

                    <h4 id="data-retrieval_world-bank">World Bank</h4>
                    <p>The World Bank datasets were collected by making requests to the World Bank API endpoints using the XML output format.</p>

                    <p>World Bank APIs was called ~150000 times to retrieve all of the WDI, WBF, WBCC, WBPO datasets, with a total disk space of ~20GB.</p>

                    <p>The data is retrieved at irregular periods - at least once a month - from the World Bank API endpoints. The retrieval act is partly based on new dataset announcements in World Bank mailing lists. Although the data retrieval and transformation phases are conducted by independent scripts, the commitment to retrieve and store the data is based on achieving the quality of the eventual RDF serialization. Therefore, Java and Bash scripts are manually executed to retrieve, in order to closely monitor abnormalities in the responses and account for necessary changes in the transformations.</p>


                    <h3 id="data-preprocessing">Data Preprocessing</h3>
                    <p>In this section, we cover some of the observed abnormalities in the original datasets, and the decisions which were made in order to later achieve reasonable RDF serializations. The information in this section is not exhaustive, and is only meant to illustrate some of the recurring challenges.</p>

                    <h4 id="data-preprocessing_cso-ireland">CSO Ireland</h4>
                    <p> The data was paged to several files that had to be pieced together, saved as CSV files, and provided with a different character encoding. Statistical data in the retrieved datasets were attributed to one of two mutually overlapping conceptualizations of Irish geography. There were datasets coded with enumeration areas and datasets coded with electoral districts. Both types of datasets were downloaded and combined during the conversion to RDF.</p>

                    <h4 id="data-preprocessing_eurostat">Eurostat</h4>
                    <p>Although, our data set parser parses TSV files to generate observation values using RDF Data Cube vocabulary but TSV files misses the frequency value which is necessary to represent observation value. In order to add frequency value to the observations of each data set, we additionally parsed SDMX format to retrieve the frequency value of each data set.</p>

                    <p>Observations with values <code>:</code> which means <em>not available</em> were left out during the RDF transformations.</p>

                    <p><code>dcterms:date</code> property was used for reference periods in Eurostat. We have encountered various reference periods and converted them into appropriate date literals while doing RDF transformation:</p>
                    <p>YYYY-MM-DD date format was used where we encountered reference periods like: <code>{YYYY}M{MM}D{DD}</code>, <code>{YYYY}Q{QQ}</code>, <code>{YYYY}M{MM}</code>, <code>{YYYY}S{SS}</code>, <code>{YYYY}</code>.</p>
                    <p>YYYY-MM-DD date format was used where we encountered reference periods like: <code>{YYYY}_{YYYY}</code>. We do not have time format information available in the data set which makes it hard to tell if the observation values are for 1 or x number of years.</p>

                    <p>Certain observation values represent reference period values as Long Term Annual Average (<dfn><abbr title="Long Term Annual Average">LTAA</abbr></dfn>). We did not know how to deal with LTAA in RDF and Eurostat does not provide any information about it in the metadata of that particular dataset. Hence, we wrote a small Turtle file which contains the definition of LTAA. Reference periods mentioning LTAA in original dataset for any particular observation value were represented in RDF using the URI of Turtle file.</p>

                    <h4 id="data-preprocessing_world-bank">World Bank</h4>
                    <p>In order to arrive at a proper and useful Linked Data representation, some of the following problems were solved either with a script or manually updated, and others were brought up to the World Bank team's attention for investigation.</p>

                    <p>For preprocessing, we've identified several recurring issues in the original data, and decided on workable solutions. They are: units of measurements in the observations were part of the indicator string were left as is; missing observation values in the API response were excluded in the transformation phase resulting in ~80% reduction in number of observations with actual measured data; various naming patterns (primarily regions) in WBF differed from WDI such that alternative names had to be added to WBF in order to arrive at canonical representations during the XSLT process; missing country codes which were identified in the WDI observations but not in the country code list was added.</p>

                    <h3 id="data-modeling">Data Modeling</h3>
                    <p>In this section we go over several areas which is at the heart of representing statistical data as Linked Data. Which vocabularies are reused and created, URI design patterns, and Data Cube's data structure definitions are discussed for each of the case studies.</p>

                    <h4 id="vocabularies">Vocabularies</h4>
                    <p>All three data models reuse the usual suspects: RDF, RDFS, XSD, OWL, XSD, FOAF, and <a href="http://www.w3.org/TR/vocab-data-cube/">RDF Data Cube</a> [<a href="#r_16">16</a>] is used to describe multi-dimensional statistical data, <a href="http://purl.org/linked-data/sdmx">SDMX</a> for the statistical information model, British reference periods (<a href="http://reference.data.gov.uk/doc/year">Year</a>, <a href="http://reference.data.gov.uk/id/gregorian-interval/">Gregorian Interval</a>), <a href="http://www.w3.org/2004/02/skos/core">SKOS</a> to describe the concepts in the observations, and <a href="http://purl.org/dc/terms/">DC Terms</a> for general purpose metadata relations.</p>

                    <p>The first versions of the converted data in CSO Ireland relied on elements from SDMX/RDF but these elements were subsequently replaced with more generic elements from the Data Cube Vocabulary (e.g., <code>qb:Observation</code> instead of <code>sdmx:Observation</code>) or SKOS (e.g., <code>skos:ConceptScheme</code> in place of <code>sdmx:code list</code>).</p>

                    <p>In the World Bank case of country codes, <a href="http://www.iso.org/iso/country_codes/background_on_iso_3166/iso_3166-2.htm">ISO 3166-2</a> [<a href="#r_17">17</a>] is used as the primary representation for countries. For example, the URI <code><a href="http://worldbank.270a.info/classification/country/CA">http://worldbank.270a.info/classification/country/CA</a></code> identifies the country <em>Canada</em> in the datasets.</p>


                    <h4 id="uri-patterns">URI Patterns</h4>
                    <p>All three datasets make use of <em>slash URIs</em> throughout the schema and data, with the exception that Eurostat uses hash URIs for its observations. The reason for this was to keep the URI patterns consistent and to make sure that all important resources when dereferenced returned information. Since the content size of the responses for statistical data may be heavy, the <em>slash URIs</em> approach appeared to be preferable to <em>hash URIs</em>, as the latter would not allow distinct requests in majority of the deployments on the Web. This is of course independent to accessing these resources via SPARQL endpoints.</p>


                    <h5 id="uri-patterns_cso-ireland">CSO Ireland</h5>
                    <p>All the resources in the 2006 Irish Census dataset use slash URIs. The geographical code lists for enumeration areas and electoral districts employ hierarchical URIs [<a href="#r_18">18</a>], in which identifiers of geographical areas are nested within the "namespace" URIs of their parent geographical features (e.g., an enumeration area nested under a county) [<a href="#r_19">19</a>].</p>

                    <p>In 2006 Irish Census dataset, blank nodes (<dfn><abbr title="blank nodes">bnodes</abbr></dfn>) were used only for identification of instances of <code>qb:ComponentSpecification</code>. All other resources were identified with URIs.</p>

                    <h5 id="uri-patterns_eurostat">Eurostat</h5>
                    <p>The base URI for Eurostat is <code>http://eurostat.linked-statistics.org/</code>. We kept same file name for the metadata and the actual dataset containing observation values as they appear in original data and distinguish them by using <code>dsd</code> and <code>data</code> in URI patterns. Further, the code lists which are shared among all datasets are provided by using <code>dic</code> in the pattern. The following URI patterns are used:</p>

                    <p id="uri-patterns_eurostat_metadata"><em>Metadata</em>: (DSDs) have the pattern <code>http://eurostat.linked-statistics.org/dsd/{id}</code>, where id is one of the dataset's metadata file.</p>
                    <p id="uri-patterns_eurostat_datasets"><em>Datasets</em>: are within <code>http://eurostat.linked-statistics.org/data/{id}</code>, where id is the filename for the dataset containing observation values.</p>
                    <p id="uri-patterns_eurostat_code-lists"><em>Code lists</em>: use the pattern <code>http://eurostat.linked-statistics.org/dic/{id}</code>, where id is the dictionary filename.</p>
                    <p id="uri-patterns_eurostat_observations"><em>Observations</em>: use the pattern <code>http://eurostat.linked-statistics.org/data/{dataset}#{dimension1},{dimensionN}</code>, where the order of dimension values in the URI space depends on the order of dimension values present in the data set.</p>

                    <h5 id="uri-patterns_world-bank">World Bank</h5>
                    <p>New URIs for classifications and properties were created because majority of the properties and concepts did not already exist in the wild, and in cases where they did, they did not fully correspond with the World Bank's. For instance, the country codes in the World Bank data are not only composed of concepts of countries, but also other geopolitical areas and income levels.</p>

                    <p>Terms in the URIs are in lower-case and delimited with the minus-sign. The dimension values are used as the terms in the URI space and are delimited with a slash.</p>

                    <p>The general URI space consists of:</p>

                    <p id="uri-patterns_world-bank_classification"><em>Classifications</em> are composed of code lists for various concepts that are used in the World Bank datasets. The concepts are compiled by using the accompanied metadata from the World Bank, and are typed with <code>skos:Concept</code>. Each code list is of type <code>skos:codeList</code> and have a URI pattern of <code>http://worldbank.270a.info/classification/{id}</code>, where id is one of; <a href="http://worldbank.270a.info/classification/country"></a>country, <a href="http://worldbank.270a.info/classification/income-level">income-level</a>, <a href="http://worldbank.270a.info/classification/indicator">indicator</a>, <a href="http://worldbank.270a.info/classification/lending-type">lending-type</a>, <a href="http://worldbank.270a.info/classification/region">region</a>, <a href="http://worldbank.270a.info/classification/source">source</a>, <a href="http://worldbank.270a.info/classification/topic">topic</a>, <a href="http://worldbank.270a.info/classification/project">project</a>, <a href="http://worldbank.270a.info/classification/currency">currency</a>, <a href="http://worldbank.270a.info/classification/loan-type">loan-type</a>, <a href="http://worldbank.270a.info/classification/loan-status">loan-status</a>, <a href="http://worldbank.270a.info/classification/variable">variable</a>, <a href="http://worldbank.270a.info/classification/global-circulation-model">global-circulation-model</a>, <a href="http://worldbank.270a.info/classification/scenario">scenario</a>, <a href="http://worldbank.270a.info/classification/basin">basin</a>. Each concept is defined under the code list namespace hierarchy e.g., <code>http://worldbank.270a.info/classification/country/CA</code> is the concept for country <em>Canada</em>.</p>

                    <p id="uri-patterns_world-bank_properties"><em>Properties</em> have the URI pattern <code>http://worldbank.270a.info/property/{id}</code>.</p>

                    <p id="uri-patterns_world-bank_datasets"><em>Data Cube datasets</em> use the URI patterns: <code>http://worldbank.270a.info/dataset/{id}</code>, where id is one of; <a href="http://worldbank.270a.info/dataset/world-development-indicators">world-development-indicators</a>, <a href="http://worldbank.270a.info/dataset/world-bank-finances">world-bank-finances</a>, <a href="http://worldbank.270a.info/dataset/world-bank-climates">world-bank-climates</a>.</p>

                    <p id="uri-patterns_world-bank_named-graphs"><em>Named graphs in RDF store</em> are placed in <code>http://worldbank.270a.info/graph/{id}</code>, where id is one of; <code>meta</code>, <code>world-development-indicators</code>, <code>world-bank-finances</code>, <code>world-bank-climates</code>, <code>world-bank-projects-and-operations</code>.</p>

                    <p id="uri-patterns_world-bank_world-development-indicators"><em>World Development Indicators</em> observations are within <code>http://worldbank.270a.info/dataset/world-development-indicators/{indicator}/{country code}/{YYYY}</code>.</p>

                    <p id="uri-patterns_world-bank_world-bank-finances"><em>World Bank Finances</em> observations are within <code>http://worldbank.270a.info/dataset/world-bank-finances/{financial dataset code}/{row id}</code>.</p>

                    <p id="uri-patterns_world-bank_world-bank-climates"><em>World Bank Climate Change</em> observations are within <code>http://worldbank.270a.info/dataset/world-bank-climates/{id}/{various patterns separated by slash}</code>.</p>

                    <p id="uri-patterns_world-bank_blank-nodes"><em>Bnodes</em>: By in large, the datasets do not contain bnodes, with the exception of unavoidable ones in the Projects and Operations code list. In order to offer a metadata file for World Bank's schema, some of the files with bnodes had to be merged. In order to avoid the conflict on collapsing bnodes with the same identifier, the decision to carry this out was based on a method that happened to be most efficient; import all of the metadata files into a named graph in RDF store, then export the named graph to a single file.</p>



                    <h4 id="data-structure-definition">Data Structure Definitions</h4>

                    <h5 id="data-structure-definition_cso-ireland">CSO Ireland</h5>
                    <p>In most cases, the data structure of the 2006 Irish Census data in RDF follows the structure of the source data, even though the transposition of the modelling patterns from the original data to its RDF version was not optimal from the perspective of modelling data in the Data Cube vocabulary. For example, for every source file representing an individual view on the Census data an instance of <code>qb:DataStructureDefinition</code> was created with dimensions that preserved the structure of the original file. Following this guideline data aggregation into a multidimensional data cube was deferred to a later stages of data processing.</p>

                    <p>Nevertheless, the guideline of preserving the original data structure was not adhered to in all of the cases. There were labels used inconsistently in the source data. For example, both "Geographical Area" and "Geographic Area" were used in the data. In such cases, where multiple labels referred to the same concept, they were merged.</p>

                    <p>The same treatment was applied on the extracted code lists that were reconstructed in RDF in a way copying their original structure. For example, multiple separate code lists for age were recreated in RDF and mapped together. A number of code lists featured "Total" as a coded value. This practice was also retained in RDF versions of these code lists.</p>

                    <h5 id="data-structure-definition_eurostat">Eurostat</h5>
                    <p>Eurostat publishes data structure definition of each dataset separately. Eurostat data structure definition consists of <em>concepts</em>, <em>code lists</em> and <em>components</em>. The <em>component</em> wraps all of the <em>concepts</em> and their associated <em>code lists</em> defined in a particular data structure definition. We defined all concepts as <code>skos:Concept</code> in our modeling approach. Further, all the code lists were defined as <code>skos:ConceptScheme</code>.</p>

                    <p>Eurostat publishes their own code lists as well as reuse SDMX code lists. For example, Eurostat uses their own custom code list to represent different indicators but reuse Frequency code lists from <a href="http://sdmx.org/wp-content/uploads/2009/01/02_sdmx_cog_annex_2_cl_2009.pdf">SDMX content-oriented guidelines</a> [<a href="#r_20">20</a>].</p>

                    <h5 id="data-structure-definition_world-bank">World Bank</h5>
                    <p>Each dataset from the World Bank was treated on a case by case basis for DSDs as they had different data models:</p>
                    <ul>
                        <li><em>World Development Indicators</em> are available as a single observation model where it contains indicators, reference areas, and time series as dimension values, and the corresponding measurement value.</li>
                        <li><em>World Bank Finances</em> come in several sub-datasets with different structures i.e., the observations in the datasets contain different set of dimensions, along with a number of measurements and attributes.</li>
                        <li><em>World Bank Climate Change</em> contains sub-datasets for different historical and future observations. They primarily include data on; reference area, reference periods, statistical types (averages and anomalies), measured variables and derived statistics, global circulation models.</li>
                    </ul>

                    <p><em>World Bank Projects and Operations</em> are treated as a code list of <a href="http://worldbank.270a.info/classification/project">projects</a>.</p>



                    <h3 id="data-conversion">Data Conversion</h3>

                    <h4 id="data-conversion_cso-ireland">CSO Ireland</h4>
                    <p>The conversion of 2006 Irish Census data was conducted with a custom Python script based on librdf [<a href="#r_21">21</a>]. Due to a relatively small volume of the data the conversion was initially implemented using an in-memory RDF store. Nevertheless, we have discovered it to be insufficient for every but the smallest datasets. Consequently, to cater for the larger datasets, the implementation switched to a file back-end based on <a href="http://www.oracle.com/technetwork/products/berkeleydb/overview/index.html">BerkeleyDB</a> [<a href="#r_22">22</a>].</p>

                    <p>Still, the back-end was not capable of handling the computation of aggregated values using SPARQL queries. To compute aggregates a more performant, standalone triple store would be needed. Ultimately, we have decided to keep the back-end and postpone generation of aggregate values to a later stage when the data would be loaded into a triple store.</p>

                    <p>Another implementation trade-off that we had to make was related to the SPARQL engine in librdf. Due to the engine not being able to execute certain types of SPARQL queries, queries were de-optimized in order for the engine to process them. For example, direct use of URIs in SPARQL graph patterns was not handled properly so that the URIs were matched in the <code>FILTER</code> clause.</p>

                    <p>The most time consuming task of the conversion regarded "reverse engineering" of code lists used in the data. Most code lists were reconstructed manually from the headers and column names in the original data. However, unlike the majority of code lists containing several items, geographical code lists were too large to be processed manually and thus it was necessary to automate their extraction. Geographical code lists, such as the enumeration areas, had to be distilled from the statistical data referring to them. Since the statistical data contained little information about the code lists (e.g., code-name pairs), it was difficult to establish identity in them (e.g., resolve name clashes).</p>

                    <h4 id="data-conversion_eurostat">Eurostat</h4>
                    <p>Various tools have been used in the publishing process: our custom-written Java program was first used to download and transform the Eurostat datasets, meta data and the code lists into RDF. Eurostat publishes more than 6100 datasets which when converted into RDF took more than 537GB of disk space yielding ~8 billion triples altogether. Instead of hosting such a huge number of triples via an SPARQL endpoint, we generated a catalog file which contains a set of triples specifying the location of a particular dataset on the file system.</p>

                    <p>The updated datasets are reflected through <code>lastUpdate</code> date field associated to each dataset in the table of contents file. We have scheduled a cron job which runs every weekend and execute the following steps:</p>

                    <ol>
                        <li>Download the new table of contents file and compare it against last week table of contents file. Compare the <code>lastUpdate</code> field from the two table of contents file for each dataset;</li>
                        <li>For all those datasets, whose <code>lastUpdate</code> field has changed, add them to the conversion list;</li>
                        <li>For each dataset in the conversion list, invoke the necessary scripts to download dataset and their associated meta data and transform them into RDF.</li>
                        <li>Replace existing RDF datasets as well as metadata with the new updated RDF datasets and metadata;</li>
                        <li>Update SPARQL endpoint;</li>
                        <li>Replace the old table of contents file with the new table of contents file for comparison in the next week cron job.</li>
                    </ol>

                    <p>The approximate time of updating Eurostat RDF datasets on weekly basis is 3-4 days depending on the number of datasets updated or added in a week time. We have also setup a mailing list which provides information on our weekly updates to the interested Eurostat users. After the update has been completed, an email is automatically sent to the mailing list describing the number of datasets which has been modified, added, and deleted.</p>

                    <p>Some of the code list names in Eurostat reuse the names in SDMX-XML codes, hence, they were kept as is since the RDF Data Cube vocabulary already uses SDMX-RDF. For other cases, they were renamed to closest possible vocabularies in RDF.</p>

                    <h4 id="data-conversion_world-bank">World Bank</h4>
                    <p>XSLT 2.0 stylesheets were created to transform the source XML files to target RDF/XML serializations. Saxon's command-line XSLT and XQuery Processor tool was used for the transformations, and employed as part of Bash scripts to iterate through all the files in the datasets. The conversion step from the command-line with <code>saxonb-xslt</code> under the Ubuntu operating system was preferred over Java's <code>SAXTransformerFactory</code> Class as it was significantly faster in preliminary tests.</p>

                    <p>In order to import this data into the RDF store rather efficiently, <a href="http://librdf.org/raptor/rapper.html">rapper</a> RDF parser utility [<a href="#r_23">23</a>] program was used to first re-serialize each RDF/XML file as N-Triples and appended to a single file at run-time before importing.</p>


                    <h3 id="linked-datasets">Linked Datasets</h3>
                    <h4 id="data-interlinking">Data Interlinking</h4>
                    <p>Interlinking things and concepts from our RDF datasets to external datasets in the LOD Cloud was a challenging task. It primarily requires the investigation of identifying eligible resource types in our datasets, and then finding suitable matches that are available externally. One useful requirement was to make sure that the target resources were dereferenable in order to make the interlinking worthwhile. Tables [<a href="#data-interlinking_cso-ireland_table">1</a>], [<a href="#data-interlinking_eurostat_table">2</a>], [<a href="#data-interlinking_world-bank_table">3</a>] gives an overview of the targeted external datasets, entity types, links, and counts for each case three cases respectively.</p>

                    <h5 id="data-interlinking_cso-ireland">CSO Ireland</h5>
                    <p>Interlinking for CSO Ireland was done by manually investigating useful URIs.</p>
                    <table id="data-interlinking_cso-ireland_table">
                        <caption><strong>Table 1.</strong> Interlinks in CSO Ireland</caption>
                        <thead><tr><th>Target dataset</th><th>Entity type</th><th>Link type</th><th>Link count</th></tr></thead>
                        <tbody>
                            <tr><td><a href="http://dbpedia.org/">DBpedia</a></td><td><code>dbo:Country</code></td><td><code>owl:sameAs</code></td><td>52</td></tr>
                            <tr><td><a href="http://dbpedia.org/">DBpedia</a></td><td><code>skos:ConceptScheme</code></td><td><code>skos:closeMatch</code></td><td>9</td></tr>
                            <tr><td><a href="http://dbpedia.org/">DBpedia</a></td><td><code>skos:Concept</code></td><td><code>skos:exactMatch</code></td><td>5</td></tr>
                            <tr><td><a href="http://sws.geonames.org/">Geonames</a></td><td><code>skos:Concept</code></td><td><code>skos:exactMatch</code></td><td>4</td></tr>
                        </tbody>
                    </table>

                    <h5 id="data-interlinking_eurostat">Eurostat</h5>
                    <p>The <a href="http://www4.wiwiss.fu-berlin.de/bizer/silk/">Silk Framework</a> [<a href="#r_24">24</a>] is used to publish initial link sets.</p>

                    <table id="data-interlinking_eurostat_table">
                        <caption><strong>Table 2.</strong> Interlinks in Eurostat</caption>
                        <thead><tr><th>Target dataset</th><th>Entity type</th><th>Link type</th><th>Link count</th></tr></thead>
                        <tbody>
                            <tr><td><a href="http://dbpedia.org/">DBpedia</a></td><td><code>dbo:Country</code></td><td><code>owl:sameAs</code></td><td>1899</td></tr>
                            <tr><td><a href="http://linkedgeodata.org/">LinkedGeoData</a></td><td><code>lgdo:Country</code></td><td><code>owl:sameAs</code></td><td>1876</td></tr>
                        </tbody>
                    </table>

                    <h5 id="data-interlinking_world-bank">World Bank</h5>
                    <p>The dataset is interlinked using <a href="http://aksw.org/Projects/limes">LInk discovery framework for MEtric Spaces</a> (<dfn><abbr title="LInk discovery framework for MEtric Spaces">LIMES</abbr></dfn>) [<a href="#r_25">25</a>]. While a great portion of the codes were matched automatically, it included a review step to catch false positives, true and false negatives, as well as curating the final results. Some of the code list concepts were manually matched with corresponding links in DBpedia using <code>skos:exactMatch</code>es and <code>skos:closeMatch</code>es, as well as to the World Bank site using <code>foaf:page</code>s.</p>

                    <table id="data-interlinking_world-bank_table">
                        <caption><strong>Table 3.</strong> Interlinks in World Bank</caption>
                        <thead><tr><th>Target dataset</th><th>Entity type</th><th>Link type</th><th>Link count</th></tr></thead>
                        <tbody>
                            <tr><td><a href="http://dbpedia.org/">DBpedia</a></td><td><code>dbo:Country</code></td><td><code>owl:sameAs</code></td><td>216</td></tr>
                            <tr><td><a href="http://dbpedia.org/">DBpedia</a></td><td><code>sdmx:Currency</code></td><td><code>owl:sameAs</code></td><td>164</td></tr>
                            <tr><td><a href="http://dbpedia.org/">DBpedia</a></td><td><code>skos:Concept</code></td><td><code>skos:exactMatch</code></td><td>5</td></tr>
                            <tr><td><a href="http://dbpedia.org/">DBpedia</a></td><td><code>skos:Concept</code></td><td><code>skos:closeMatch</code></td><td>3</td></tr>
                            <tr><td><a href="http://eurostat.linked-statistics.org/">Eurostat</a></td><td><code>dbo:Country</code></td><td><code>owl:sameAs</code></td><td>216</td></tr>
                            <tr><td><a href="http://worldbank.org/">World Bank</a></td><td><code>foaf:Document</code></td><td><code>foaf:page</code></td><td>119526</td></tr>
                        </tbody>
                    </table>


                    <h4 id="data-enrichment">Data Enrichment</h4>
                    <p>In this section we talk about some of the ways that we've tried to enrich the original data by adding information such that the datasets can be more useful, easily discovered, interlinked or queried.</p>

                    <h5 id="data-enrichment_cso-ireland">CSO Ireland</h5>
                    <p>The only data enrichment for CSO Ireland was the addition of World Geodetic System <dfn><abbr title="World Geodetic System">WGS</abbr></dfn> triples using <code>rdf:type</code> of <code>wgs:Point</code>, as well as latitude and longitude using <code>wgs:lat</code> and <code>wgs:long</code> to the descriptions of Irish city resources.</p>

                    <h5 id="data-enrichment_eurostat">Eurostat</h5>
                    <p><code>rdf:datatype</code>s were added to observation values in each dataset. The reason is some datasets has combination of numeric and decimal values which makes it harder to query or the user has to cast the datatype for observation values with different data types before making query. To avoid data type casting at query level, we pre processed all observation values of a dataset and associated the appropriate data type to all observation values in a particular dataset before serializing it into RDF.</p>

                    <h5 id="data-enrichment_world-bank">World Bank</h5>
                    <p>A code list for currencies was created based on <a href="http://www.currency-iso.org/iso_index/iso_tables/iso_tables_a1.htm">ISO 4217</a> currency and funds name [<a href="#r_26">26</a>] to represent the SDMX attributes for the amount measurements in the World Bank Finances datasets. They were also linked to each country which officially uses that currency. Given that some of the codes in the World Bank country code list are not considered to be countries e.g., <code>1W</code> representing <em>World</em>, only the resources which represented a real country have an added <code>rdf:type</code> instance of <code>dbo:Country</code>.</p>



                    <h4 id="rdf-datasets">RDF Datasets</h4>
                    <p>Tables [<a href="#linked-dataset_cso-ireland_table">4</a>], [<a href="#linked-dataset_eurostat_table">5</a>], [<a href="#linked-dataset_world-bank_table">6</a>] outlines the current state of RDF Linked Datasets for CSO Ireland, Eurostat, and World Bank. The size of the dataset are in rounded number of triples. The <a href="#void">datasets' VoID files</a> would normally contain exact numbers.</p>


                    <table id="linked-dataset_cso-ireland_table">
                        <caption><strong>Table 4.</strong> CSO Ireland Linked Data</caption>
                        <thead><tr><th>Dataset</th><th>Format</th><th>Size</th><th>Number of triples</th><th>Number of observations</th></tr></thead>
                        <tbody>
                            <tr><th>2006 Irish Census</th><td>Turtle</td><td>776MB</td><td>12M</td><td>1.6M</td></tr>
                        </tbody>
                    </table>

                    <table id="linked-dataset_eurostat_table">
                        <caption><strong>Table 5.</strong> Eurostat Linked Data</caption>
                        <thead><tr><th>Dataset</th><th>Format</th><th>Size</th><th>Number of triples</th><th>Number of observations</th></tr></thead>
                        <tbody>
                            <tr><th>Datasets</th><td>RDF/XML</td><td>537GB</td><td>8B</td><td>1B</td></tr>
                            <tr><th>DSDs</th><td>Turtle</td><td>347MB</td><td>6M</td><td>N/A</td></tr>
                            <tr><th>Code lists</th><td>RDF/XML</td><td>21MB</td><td>0.3M</td><td>N/A</td></tr>
                        </tbody>
                    </table>

                    <table id="linked-dataset_world-bank_table">
                        <caption><strong>Table 6.</strong> World Bank Linked Data</caption>
                        <thead><tr><th>Dataset</th><th>Format</th><th>Size</th><th>Number of triples</th><th>Number of observations</th></tr></thead>
                        <tbody>
                            <tr><th>Climate Change</th><td>RDF/XML</td><td>10GB</td><td>78M</td><td>7M</td></tr>
                            <tr><th>Development Indicators</th><td>RDF/XML</td><td>8.4GB</td><td>79M</td><td>11M</td></tr>
                            <tr><th>Finances</th><td>RDF/XML</td><td>827MB</td><td>7M</td><td>0.25M</td></tr>
                            <tr><th>Projects and Operations</th><td>RDF/XML</td><td>93MB</td><td>0.96M</td><td>N/A</td></tr>
                        </tbody>
                    </table>


                    <h3 id="data-loading-to-store">Loading RDF data to RDF store</h3>
                    <h4 id="data-loading-to-store_cso-ireland">CSO Ireland</h4>
                    <p>As the data model for CSO Ireland was improved over time, the RDF store had to be loaded several times. In the earlier versions of Fuseki and its TDB RDF store, the RDF store also had to be rebuilt in order to optimize query responses accordingly. GraphPusher was used to help automate bulk-loading. As the size of the 2006 Irish Census RDF data dumps are under 15MB compressed in total, downloading of the dumps to a temporary location multiple times raised no issues.</p>

                    <h4 id="data-loading-to-store_eurostat">Eurostat</h4>
                    <p>The sheer large size of Eurostat's RDF data dumps (~533GB RDF/XML) presented a challenge to load them into an RDF store given that the resulting store size would have been multiple factors greater than the original dumps. Even if the load time, as well as space use was put to the side, querying over such a large dataset would have been difficult to optimize for reasonable response times.</p>

                    <h4 id="data-loading-to-store_world-bank">World Bank</h4>
                    <p>Given the current scope of GraphPusher for not being able to work with local files, it wasn't suitable for it to download the World Bank's RDF data dumps repeatedly, since its compressed size was around 400MB. However, simple bash scripts was written to load the data directly to TDB store. The process to load the files was done in two steps:</p>
                    <ol>
                        <li>Each RDF file which belonged to the same named graph was transformed to N-Triples format using the rapper utility, and then concatenated to a single N-Triples file. The cost of this extra transformation reduced the load time into the TDB store.</li>
                        <li>Loading of each N-Triples files to corresponding graphs.</li>
                    </ol>

                    <p>Files which were known to contain bnodes was not merged as they posed a conflict, and hence loaded individually.</p>



                    <h3 id="data-license">Data License</h3>
                    <h4 id="data-license_cso-ireland">CSO Ireland</h4>
                    <p>The use of the 2006 Irish Census data is governed by the Re-use of <a href="http://www.cso.ie/en/aboutus/dissemination/re-useofpublicsectorinformation/">Public Sector Information Statutory Instrument 279/2005</a> [<a href="#r_27">27</a>], which allows reuse provided that the creator (i.e., CSO) is attributed, the information is represented in an accurate way, and the information is used for a non-commercial purpose.</p>
                    <h4 id="data-license_eurostat">Eurostat</h4>
                    <p>The actual Eurostat datasets adhere to <a href="http://epp.eurostat.ec.europa.eu/portal/page/portal/about_eurostat/policies/copyright_licence_policy">Eurostat's terms of use</a> [<a href="#r_28">28</a>] while the RDF data is licensed under <a href="http://creativecommons.org/publicdomain/zero/1.0/">CC0 1.0 Universal (CC0 1.0) Public Domain Dedication</a> [<a href="#r_29">29</a>].</p>
                    <h4 id="data-license_world-bank">World Bank</h4>
                    <p>In addition to adhering to <a href="http://go.worldbank.org/OJC02YMLA0">World Bank's terms of use</a> [<a href="#r_30">30</a>], the RDF data that is published is licensed under CC0 1.0 Universal (CC0 1.0) Public Domain Dedication.</p>


                    <h3 id="publication">Publication</h3>
                    <p>In this section we talk about various ways of making our Linked Datasets publicly accessible, discoverable, and usable.</p>

                    <h4 id="data-provenance">Data Provenance</h4>
                    <p>Table <a href="data-provenance_table">7</a> outlines the vocabulary terms that are used in particular to provenance information in all three case studies. The provenance metadata for 2006 Irish Census was not created during the process of conversion to RDF. Some of the properties were added to the VoID file at a later stage.</p>

                    <table id="data-provenance_table">
                        <caption><strong>Table 7.</strong> Provenance in CSO Ireland, Eurostat and World Bank Linked Datasets</caption>
                        <thead><tr><th>Type of provenance</th><th>CSO Ireland</th><th>Eurostat</th><th>World Bank</th></tr></thead>
                        <tbody>
                            <tr><th>Defining source</th><td></td><td></td><td><code>rdfs:isDefinedBy</code></td></tr>
                            <tr><th>License</th><td></td><td><code>rdfs:seeAlso</code></td><td><code>dcterms:license</code></td></tr>
                            <tr><th>Source location</th><td><code>dcterms:source</code></td><td><code>dcterms:source</code></td><td><code>dcterms:source</code></td></tr>
                            <tr><th>Related resource</th><td></td><td></td><td><code>dcterms:hasPart</code>, <code>dcterms:isPartOf</code></td></tr>
                            <tr><th>Creator of the data</th><td><code>dcterms:creator</code></td><td><code>foaf:maker</code></td><td><code>dcterms:creator</code></td></tr>
                            <tr><th>Publisher of the data</th><td></td><td></td><td><code>dcterms:publisher</code></td></tr>
                            <tr><th>Creation date</th><td><code>dcterms:created</code></td><td><code>dcterms:created</code></td><td><code>dcterms:created</code></td></tr>
                            <tr><th>Issued date</th><td></td><td></td><td><code>dcterms:issued</code></td></tr>
                            <tr><th>Modified date</th><td></td><td><code>dcterms:modified</code></td><td><code>dcterms:modified</code></td></tr>
                        </tbody>
                    </table>



                    <h4 id="void">VoID</h4>
                    <p>We use describe the Linked Datasets with the <a href="http://www.w3.org/TR/void/">VoID</a> vocabulary [<a href="#r_31">31</a>]. It is generally intended to give an overview of the dataset metadata i.e., what it contains, ways to access it or query it.</p>

                    <h5 id="void_cso-ireland">CSO Ireland</h5>
                    <p>A <a href="http://data-gov.ie/.well-known/void">VoID</a> file is compiled for the CSO Ireland datasets [<a href="#r_32">32</a>]. The information included, but not limited to is, locations to RDF datadumps, named graphs that are used in the SPARQL endpoint, used vocabularies, size of the datasets, and interlinks to external datasets.</p>
                    <h5 id="void_eurostat">Eurostat</h5>
                    <p>The VoID file is planned for next Eurostat Linked Data release.</p>
                    <h5 id="void_world-bank">World Bank</h5>
                    <p>A <a href="http://worldbank.270a.info/.well-known/void">VoID</a> file is compiled for the World Bank Linked Datasets [<a href="#r_33">33</a>]. The information included, but not limited to is, locations to RDF datadumps, named graphs that are used in the SPARQL endpoint, used vocabularies, size of the datasets, and interlinks to external datasets. Granular set of dataset statistics in generated and also included in the VoID file using <a href="http://aksw.org/projects/LODStats">LODStats</a> [<a href="#r_34">34</a>].</p>


                    <h4 id="user-interface">User-interface</h4>
                    <p>Here we briefly discuss how the published data can be access primarily using a Web browser.</p>
                    <h5 id="user-interface_cso-ireland">CSO Ireland</h5>
                    <p>The website is located at <code><a href="http://data-gov.ie/">http://data-gov.ie/</a></code> [<a href="#r_35">35</a>]. The HTML pages are generated and published by the <a href="https://github.com/csarven/linked-data-pages">Linked Data Pages</a> framework [<a href="#r_36">36</a>]. Linked Data Pages is used to invoke unique SPARQL queries based on the requested URI. The results are outputted in corresponding HTML templates. Links to alternate RDF formats as well as in JSON are handled by content-negotiation. Given the nature of the invoked SPARQL query, alternate formats may contain additional triples like labels for the vocabulary terms that&#8217;s not in the RDF dumps. This minor difference is mentioned for the users on the site. For some resources e.g., Cities, <a href="https://google-developers.appspot.com/chart/">Google Charts Tools</a> [<a href="#r_37">37</a>] is used to display various visualizations in place of the tabular data in the HTML.</p>
                    <h5 id="user-interface_eurostat">Eurostat</h5>
                    <p>The website is located at <code><a href="http://eurostat.linked-statistics.org/">http://eurostat.linked-statistics.org/</a></code> [<a href="#r_38">38</a>] where it contains custom static HTML pages, with links to dump directories, and SPARQL endpoint.</p>

                    <h5 id="user-interface_world-bank">World Bank</h5>
                    <p>The website is located at <code><a href="http://worldbank.270a.info/">http://worldbank.270a.info/</a></code> [<a href="#r_39">39</a>]. The publication of approach of the World Bank Linked Data is same as CSO Ireland.</p>



                    <h4 id="sparql-endpoint">SPARQL Endpoint</h4>
<p class="todo">Public, Performance (optimization with TDB)</p>
                    <p>The SPARQL endpoints are accessible publicly with no authorization requirements. As a trade-off, the SPARQL service is subject to high resource demands from the server for certain query types. No query restrictions were placed in neither of the cases.</p>
                    <h5 id="sparql-endpoint_cso-ireland">CSO Ireland</h5>
                    <p>Same setup as World Bank with its own <a href="http://data-gov.ie/sparql">SPARQL endpoint</a>. The endpoint allows access to the full schema and datasets, and uses named graphs.</p>
                    <h5 id="sparql-endpoint_eurostat">Eurostat</h5>
                    <p>Same setup as World Bank with its own <a href="http://eurostat.linked-statistics.org/sparql">SPARQL endpoint</a>. This endpoint includes only the schema and excludes the data (~533GB RDF/XML) due to the limitation of the available resources and performance reasons.</p>
                    <h5 id="sparql-endpoint_world-bank">World Bank</h5>
                    <p>Apache Jena&#8217;s <a href="http://incubator.apache.org/jena/documentation/tdb/">TDB</a> [<a href="#r_40">40</a>] storage system and <a href="http://incubator.apache.org/jena/documentation/serving_data/index.html">Fuseki</a> [<a href="#r_41">41</a>] is used to run the SPARQL server. Data in RDF format was incrementally loaded from into TDB RDF store using Jena's <code>tdbloader</code> script from command-line. A public <a href="http://worldbank.270a.info/sparql">SPARQL endpoint</a> is available which accepts SPARQL 1.1 queries. The endpoint allows access to the full schema and datasets, and uses named graphs.</p>


                    <h4 id="data-dumps">Data Dumps</h4>
                    <p>The data dumps come in different formats, primarily either available in native RDF formats, or they are compressed for easy retrieval. Dumps are usually made visible by a link on the sites, or mentioned in the VoiD files. They are also <a href="#dataset-announcement">announced in the Data Hub</a>.</p>
                    <h5 id="data-dumps_cso-ireland">CSO Ireland</h5>
                    <p>The <a href="http://data-gov.ie/data/">CSO Ireland's RDF data dumps</a> [<a href="#r_42">42</a>] are available in RDF Turtle compressed with gzip.</p>
                    <h5 id="data-dumps_eurostat">Eurostat</h5>
                    <p>The <a href="http://eurostat.linked-statistics.org/data/">Eurostat's RDF data dumps</a> [<a href="#r_43">43</a>] are available as individual RDF/XML files from the file system.</p>
                    <h5 id="data-dumps_world-bank">World Bank</h5>
                    <p>The <a href="http://worldbank.270a.info/data/">World Bank's RDF data dumps</a> [<a href="#r_44">44</a>] are available either as individual RDF/XML files or in compressed gzip format.</p>

                    <h3 id="code">Source Code</h3>
                    <h4 id="code_world-bank">World Bank</h4>
                    <p>The code which retrieves the World Bank data, transforms it to RDF serializations, and imports to TDB Triple Store can be found at GitHub: <a href="https://github.com/csarven/worldbank-linkeddata">csarven/worldbank-linkeddata</a> [<a href="#r_50">50</a>]. It is using the <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License 2.0</a>.</p>


                    <h3 id="dataset-announcement">Announcing the Datasets</h3>
                    <h4 id="dataset-announcement_world-bank">World Bank</h4>
                    <p>The dataset is registered in the Data Hub with ID: <a href="http://thedatahub.org/dataset/world-bank-linked-data">world-bank-linked-data</a> [<a href="#r_47">47</a>]. The dataset <a href="http://www4.wiwiss.fu-berlin.de/lodcloud/ckan/validator/validate.php?package=world-bank-linked-data">has level 3</a> in the CKAN Validator. It is a candidate for the lodcloud group.</p>



<p class="todo">Create a table comparing datasets in the LOD cloud... vs. average per domain. Assumption is that statistical linked dataspace has a low density of interlinks</p>



                    <h2>Conclusions</h2>

                    <div id="references">
                        <h2>References</h2>
                        <ol about="[this:]">
<!--
<li id="r_2">Ngonga Ngomo, A.-C.: <em>A Time-Efficient Hybrid Approach to Link Discovery</em>, The Sixth International Workshop on Ontology Matching, ISWC (2011), <a rel="dcterms:references" href="http://www.dit.unitn.it/~p2p/OM-2011/om2011_Tpaper1.pdf">http://www.dit.unitn.it/~p2p/OM-2011/om2011_Tpaper1.pdf">http://www.dit.unitn.it/~p2p/OM-2011/om2011_Tpaper1.pdf</a></li>
-->
                        </ol>
                    </div>
                </div>
            </div>
        </div>
    </body>
</html>
